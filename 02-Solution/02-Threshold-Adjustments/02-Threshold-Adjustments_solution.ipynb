{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f92971",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-cL5eOpEsbuIEkvwW2KnpXC12-PAbamr\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffaf6bd",
   "metadata": {},
   "source": [
    "# Threshold Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a874e0de",
   "metadata": {},
   "source": [
    "The data you will be working with in this exercise consists of measurements of the ionosphere using radar. The radar data was collected by a system in Goose Bay, Labrador. This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts. The targets were free electrons in the ionosphere. \"Good\" radar returns are those showing evidence of some type of structure in the ionosphere. \"Bad\" returns are those that do not; their signals pass through the ionosphere.\n",
    "\n",
    "Received signals were processed using an autocorrelation function whose arguments are the time of a pulse and the pulse number. There were 17 pulse numbers for the Goose Bay system. Instances in this databse are described by 2 attributes per pulse number, corresponding to the complex values returned by the function resulting from the complex electromagnetic signal.\n",
    "\n",
    "If you are curious, more details about this dataset<a href='https://archive.ics.uci.edu/ml/datasets/Ionosphere'> can be found on the UCI machine learning website.</a>\n",
    "\n",
    "üëá Load the player `ionosphere.data` dataset located within the data folder to see what you will be working with. Note that the dataset does **NOT** have headers for the columns, so you need use <code>header=None</code> as an argument in your <code>read_csv</code> method. <br>\n",
    "Call your new dataframe <code>data</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='1aT-kqzDPaNZCG82NMGmbjwUtUFbtGLZP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731595bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('raw_data/ionosphere.data', header=None)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b547060",
   "metadata": {},
   "source": [
    "‚ÑπÔ∏è There is a total of 34 features in this dataset:\n",
    "* All 34 are continuous\n",
    "* The 35th attribute is either \"good\" (g) or \"bad\" (b) according to the definition summarized above. \n",
    "\n",
    "Hence, this is a binary classification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b098fe9e",
   "metadata": {},
   "source": [
    "# Preparing the target\n",
    "Your first task will be to encode the target using <code>sklearn LabelEncoder</code>. Do this, create a new column in your dataset named <code>y</code> and remove the original label column (<code>34</code>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92107611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "data['y'] = label_encoder.fit_transform(data[34])\n",
    "data.drop(34, inplace=True, axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916042ab",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c76ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('target_encoding',\n",
    "                         dataset = data\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766fe75b",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e27c1",
   "metadata": {},
   "source": [
    "üëá This dataset has no missing values (you're welcome). So first create a target dataset (<code>y</code>) and a feature dataset <code>X</code>. Then let's go ahead and train/test split these using a <code>random_state=42</code> and a <code>test_size=0.3</code> this way your results will be comparable (name them <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, <code>y_test</code>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b5241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data.y.copy()\n",
    "X = data.drop('y', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be4a5e0",
   "metadata": {},
   "source": [
    "üëá To avoid spending too much time on the preprocessing, Robust Scale the entire feature set. This practice is not optimal, but can be used for preliminary preprocessing and/or to get models up and running quickly. Remember to train your <code>RobustScaler()</code> on the <code>X_train</code> only, and then use the <code>.transform</code> method on both the <code>X_train</code> and <code>X_test</code> dataset (replaced the original by the scaled version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4a934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler().fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f45d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f97749e",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f5b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('scaled_features',\n",
    "                         scaled_features = X_train\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94b7da7",
   "metadata": {},
   "source": [
    "# Base modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e600be",
   "metadata": {},
   "source": [
    "üéØ The task is to detect good radar readings with a 90% guarantee.\n",
    "\n",
    "First, let's create a dummy model to see what our accuracy would be if we took a random guess and always classified an instance as a 'bad' reading. Checkout the <code>scikit-learn</code> documentation for <code>DummyClassifier</code> and <code>precision_score</code>, train your classified on your <code>X_train</code> and save the <code>precision_score</code> of your predictions under a variable named '<code>dummy_baseline</code>'.üëá "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8575213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "dummy = DummyClassifier()\n",
    "\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "dummy_predictions = dummy.predict(X_test)\n",
    "\n",
    "dummy_baseline = precision_score(y_test, dummy_predictions)\n",
    "dummy_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fc1007",
   "metadata": {},
   "source": [
    "Well, that is interesing isn't it? Are you surprise by this baseline score? Explain in a sentence or two below what you think this result means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f86416",
   "metadata": {},
   "outputs": [],
   "source": [
    "'The score is >50% which means that the dataset must be slightly unbalanced towards class 1 (good measurements)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aea59a",
   "metadata": {},
   "source": [
    "Can you test your theory by looking at the dataset? üëá "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104f2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdd5cd8",
   "metadata": {},
   "source": [
    "üëá Now let's check if a default Logistic Regression model is going to satisfy our requirement of over 90% precision. Use cross validation on your <code>X_train</code> and save the score that supports your answer under variable name `base_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df12a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "base_model = LogisticRegression(max_iter=2000)\n",
    "\n",
    "metrics = ['precision', 'recall']\n",
    "\n",
    "cv = cross_validate(base_model, X_train, y_train, scoring=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_score = np.mean(cv['test_precision'])\n",
    "base_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c272c",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b9cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('base_precision',\n",
    "                         score = base_score,\n",
    "                         dummy = dummy_baseline\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac769d1d",
   "metadata": {},
   "source": [
    "# Threshold adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2a3020",
   "metadata": {},
   "source": [
    "So our logistic regression does much better than a DummyClassifier. ü•≥ But this is still not quite where we need it (>90%). Luckily, because we are dealing with binary classification we can adjust our decision threshold to increase precision at the cost of accuracy.<br>\n",
    "üëá Find the decision threshold that guarantees a 90% precision for a positive identification as belonging to the 'good measurement' class. Save the threshold under variable name `new_threshold`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c41a90f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "- Make cross validated probability predictions with [`cross_val_predict`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html)\n",
    "    \n",
    "- Plug the probabilities into [`precision_recall_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html) to generate precision scores at different thresholds\n",
    "\n",
    "- Find out which threshold guarantees a precision of 0.9\n",
    "      \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "proba_neg, proba_pos = cross_val_predict(LogisticRegression(),\n",
    "                                         X_train,\n",
    "                                         y_train, \n",
    "                                        method='predict_proba',\n",
    "                                           cv=5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f07c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, threshold = precision_recall_curve(y_train,proba_pos, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame({'Threshold':threshold,\n",
    "                     'Precision':precision[:-1],\n",
    "                     'Recall':recall[:-1]})\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1742d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[scores.Precision > 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = scores[scores.Precision > 0.9].index[0]\n",
    "new_threshold = scores.loc[idx,'Threshold']\n",
    "new_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08138b5e",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08dad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('decision_threshold',\n",
    "                         threshold = new_threshold\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3617e0c8",
   "metadata": {},
   "source": [
    "# Using the new threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee969580",
   "metadata": {},
   "source": [
    "üéØ Now let's properly train our <code>LogisticRegression()</code> model using the train set, test it with the test set and the <code>precision_score</code> using your new threshold. Remember that you will need to use the <code>.predict_proba</code> method on your logistic classifier and apply the threshold manually. Save the precision on the test score as a variable named <code>test_precision_score</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1992f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression().fit(X_train, y_train)\n",
    "proba_classes = model.predict_proba(X_test)\n",
    "y_predict = []\n",
    "\n",
    "for proba_0, proba_1 in proba_classes:\n",
    "    if proba_1 >= new_threshold:\n",
    "        y_predict.append(1)\n",
    "    else:\n",
    "        y_predict.append(0)\n",
    "\n",
    "test_precision_score = precision_score(y_test,y_predict)\n",
    "test_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2daf1a",
   "metadata": {},
   "source": [
    "ü§î So this is not quite 90% is it? This is because we adjusted the threshold on a small training set. As you can see, on unseen data this does not quite translate to exactly what we wanted, but it is close and better than before!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904aedfb",
   "metadata": {},
   "source": [
    "‚ùì Now let's open a new, unseen sample without a label: open the <code>ionosphere_sample.csv</code> file located in your <code>data</code> folder (remember: <code>header=False</code>). Do you think that this is a good or a bad reading? Save your answer as string under variable name `recommendation` as \"good\" or \"bad\". <br>\n",
    "üö® Remember to scale this data with your scaler before you predict (you will need to transpose the data as you have only 1 sample in a column vector)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('raw_data/ionosphere_sample.csv', header=None)\n",
    "sample_scaled = scaler.transform(sample.T)\n",
    "model.predict_proba(sample_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3038cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation = 'good'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a38045b",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed58a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('recommendation',\n",
    "                         recommendation = recommendation\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f200bb4b",
   "metadata": {},
   "source": [
    "# üèÅ Finished!\n",
    "\n",
    "Well done! <span style=\"color:teal\">**Push your exercise to GitHub**</span>, and move on to the next one."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
