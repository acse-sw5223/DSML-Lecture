{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a554df35",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-d7H1l1lJ28_sLcd9Vvh_N-yro7CJZcZ\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c4632",
   "metadata": {},
   "source": [
    "# Golden Plains Roadside Biodiversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7a3842",
   "metadata": {},
   "source": [
    "This is your first data problem! Remember, \"Data Problems\" are a little bit less directed than the skills problem. They are here to encourage you to use your critical thinking when dealing with data. It is also a better reflection of the type of problems you will encounter during your assessed coursework at the end of the course. Make sure you understand what you have done in the previous exercises, and apply it here. Also, ***get into the habit of maintaining a  clean, working notebook***. This will be a key assessment criteria for your marked coursework later next week, so take this opportunity (and further ones) to learn how to do this. This includes using `markdown` cells for comments and observations, making sure your code can run from top to bottom when using `run all cells` from the menu, and of course, keeping a **clean code** practice. It also also a good idea, once you are done with your work, to put all of your `import` statements at the top of the notebook: this way, it is clear what is imported in the entire notebook and allows you to focus on your more important code below.\n",
    "\n",
    "Here is a little bit of information on the data you are given. Golden Plains Shire (Australia) is responsible for managing 1834 kilometres of road reserves. Road reserves are not only used for transport but also act as service corridors, in fire prevention, recreation, and occasionally agricultural pursuits. Native vegetation on roadsides is important flora and fauna habitat and landscape character.\n",
    "\n",
    "In 2014, Golden Plains Shire acquired funding through the Victorian Adaptation and Sustainability Partnership (VASP) to undertake Councils ‚ÄòBuilding Adaptive Capacity on Roadsides‚Äô project. The Project was designed to identify significant environmental assets on roadsides, improve roadside management practices and reduce Council‚Äôs risk of potential breaches against Federal and State environmental legislation. \n",
    "\n",
    "The council made this <a href='https://data.gov.au/data/dataset/golden-plains-roadside-biodiversity'>dataset available here</a>.<br>\n",
    "![plain](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6b/Mount_Conner%2C_August_2003.jpg/375px-Mount_Conner%2C_August_2003.jpg)\n",
    "<br>\n",
    "\n",
    "üéØ Today, you will work with a simplified version of this real dataset. The dataset contains a number of biodiversity observations including one on tree size (`RCACTreesS`). This exercise consists of the data preparation and modelling techniques you have learnt: our goal is to predict via linear regression the `RCACTreesS` using the available features and obtain a good score.\n",
    "\n",
    "‚ö†Ô∏è This is a long exercises, which will require you to think about the data. Don't hesitate to plot things - if you need to use algorithm that use a `random_seed` such as `train_test_split` or others, remember to always use the value `42` so your results can be compared to the proposed solution. If you get stuck, ask a TA!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcfaf49",
   "metadata": {},
   "source": [
    "# Part I: Ensuring Generalization and EDA\n",
    "\n",
    "In this first part, do the following:\n",
    "1. üëá Load the data into this notebook as a pandas dataframe named `df`, and display its first 5 rows.\n",
    "2. Check for and drop duplicates\n",
    "3. We will use the `RCACTreesS` as our target variable (`y`) and all other columns as our features (`X`).\n",
    "4. Split the dataset into 80%/20% train/test splits (use a `random_state=42`) to create your `X_train`, `X_test`, `y_train`, `y_test` (see above regarding the `y`).\n",
    "5. **Using only the X_train**, spend some time exploring the dataset, for instance looking at the different columns it contains, it's data types, any missing values. Check for correlations between features, and draw some plots. At the end of this EDA stage, you should have a good idea of what the data is. Try to keep this notebook cleanly organised, using `Markdown` cells to put comments for yourself (and your TAs) about your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad7ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='19qi8xMUaamIAX8KcZproR33c2JQcOAul')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e07f02f",
   "metadata": {},
   "source": [
    "# All imports below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0385f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE DATA HERE\n",
    "df = pd.read_csv('raw_data/biodiversity.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d64d10",
   "metadata": {},
   "source": [
    "Spend a bit of time exploring the dataset, for instance looking at the different columns it contains, it's data types, any missing values. You could use the `describe()` function as a starting point to have an idea of what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a35b5",
   "metadata": {},
   "source": [
    "# Droping Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0513a22",
   "metadata": {},
   "source": [
    "Checking for duplicates, and removing them  from the dataset. I overwite the dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc9ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48c4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop them in place and check\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c141b13",
   "metadata": {},
   "source": [
    "# Splitting the dataset\n",
    "\n",
    "Now that the duplicates have been removed, there is no longer a risk of data leakage and we can split the data. I want to do this **BEFORE** I look at any of the statistics so my eye is not influenced by values in the `X_test` that I should not have seen. From then on, I will ignore my `X_test` (and my original `df` that contains the `X_test` data) and focus all my work only on the `X_train`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ccc3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='RCACTreesS'),df.RCACTreesS, train_size=.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c6372f",
   "metadata": {},
   "source": [
    "Let's look at the spread of numerical features using `describe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab2692",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416fc4a8",
   "metadata": {},
   "source": [
    "### Correlation matrix\n",
    "\n",
    "It is also a good idea to look at the correlations between features - are any highly correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd75d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "corr = X_train.select_dtypes(['Int64', 'Float64']).corr() # Only selecting the numerical values here\n",
    "\n",
    "sns.heatmap(corr, cmap='seismic', ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac361f7",
   "metadata": {},
   "source": [
    "### We can also look at the data types and explore missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1763dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make my life easier, I create a DataFrame of data types\n",
    "data_types = pd.DataFrame(X_train.dtypes, columns=['Data Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95108f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the different data types in my collections:\n",
    "data_types.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb615ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the categorical data\n",
    "X_train.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the numerical data\n",
    "\n",
    "X_train.select_dtypes(['int64','float64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f013292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There seems to be a lot of empty values. Let's explore this:\n",
    "pct_empty = X_train.isnull().sum().sort_values(ascending=False)/X_train.shape[0]\n",
    "pct_empty[pct_empty>.3] # Only displaying the values with lots of empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77d89ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_empty[pct_empty<.3] # Only displaying the values with less than 30% empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e5314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can calculate the proportion of features that hvae <30% missing values:\n",
    "pct_empty[pct_empty<.3].shape[0]/pct_empty.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac8430f",
   "metadata": {},
   "source": [
    "## What we learned\n",
    "\n",
    "We already know the following now:\n",
    "1. The data has numerical values and categorical values\n",
    "2. Some of the numerical values are correlated\n",
    "3. There are some features with >30% missing values but 76% of the features are well represented (not bad!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132b3a2",
   "metadata": {},
   "source": [
    "# Part II: Missing values and scaling\n",
    "\n",
    "Now do the following:\n",
    "1. Drop features with >30% missing values\n",
    "2. Imput `RoadWidthM`, `PowerlineD` and `Trees` using the most appropriate strategy <details>\n",
    "    <summary> üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è Look at the datatype of <code>PowerlineD</code> and the distribution of the data using the <code>.unique()</code> method. Although <code>PowerlineD</code> is a numeric value, it clearly only has discrete distribution: what would be a logical value to impute? The same applies to <code>Trees</code> and <code>RoadWidthM</code> but for a different reason: they are a continuous variable but there is clearly one value that dominates the distribution: it makes sense to assume that the `nan` represent this most frequent value. So you can impute both of these variables at the same time.\n",
    "</details> \n",
    "3. Imput `Locality` and `EVNotes` <details>\n",
    "    <summary>üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è Clearly <code>Locality</code> refers to the name of the county or region where the data comes from. We could impute the most frequent locality, but this would induce some errors. In this case, the best strategy is simply to replace the <code>nan</code> by something meaningful such as 'not known'. <code>EVCNotes</code> is somewhat similar: the <code>nan</code> values indicate that no notes exist, so we should replace them by 'no notes'.\n",
    "</details>\n",
    "4. Impute `SoilType` and `LandformLS` <details>\n",
    "    <summary>üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è These two are tricky. They both are string values, and they both have two classes that are very common. On a real project, a good data scientist will study what those codes means <a href=\"http://vro.agriculture.vic.gov.au/dpi/vro/vrosite.nsf/pages/landform_land_systems_rees/$FILE/TECH_56%20ch6.pdf\"> by refering to the government publication</a>. In an ideal world we would explore different strategies for imputation (we will see this later in the course). However here we need to decide based on little evidence. Because we have no information, and because there is not a clear majority in either soil or landform classes, the best is to impute 'SoilTypeNA' and 'LandFormLSNA' as as a new class.\n",
    "</details>\n",
    "5. Imput `CanopyCont` <details>\n",
    "    <summary>üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è If you do a <code>value_counts()</code> on <code>CanopyCont</code> you will see that this consists of 4 numerical variables, and 5 categorical variables. It is clear that this column has two different encoding for the same concept: how continuous is the canopy? The easiest is to transform this into a numerical column by doing the following replacements: 'none'=0, 'sparse'=1, 'patchy'=2, 'continous' or 'c' = 3. You probably want to use a python dictionary and an <code>apply()</code> function to do that, and remember to cast your values to an <code>int</code> or a <code>float</code>!\n",
    "</details>\n",
    "6. Scale all of your numerical features using an appropriate scaler. Check their distribution before deciding on your scaling strategy! <details>\n",
    "    <summary>üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è <code>WidthVarie</code>, & <code>Powerline</code> are clearly binary variable ([0,1]). They should not be scaled, but rater can optionally be encoded using a <code>CategoricalEncoder</code>. Simply leave them as they are. All other numerical features are non-guassian so a `RobustScaler` is probably the most appropriate.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e057da1",
   "metadata": {},
   "source": [
    "# Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd14bf6",
   "metadata": {},
   "source": [
    "üëá Locate missing values, investigate them, and apply the solutions below accordingly:\n",
    "\n",
    "- Impute with most frequent\n",
    "- Impute with median\n",
    "- Impute a different value which makes sense for the particular data\n",
    "\n",
    "Make changes effective in the dataset `X_train`. Hints are provided to guide you along in your decision, but before using the hint, try to come up with your own strategy by plotting a historgram of distribution of your variables, or looking a a `value_counts()` output. Trying on your own before looking at the hint is important to your learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b9551",
   "metadata": {},
   "source": [
    "## `Features with >30% missing data`\n",
    "\n",
    "Identify all features where the amount of missing data is >30% and deal with it approrpiately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d07cf",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary> üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è The easiest way to do this is to first create a series containing the percentage of missing values, then filter this for values > 30%, and obtain from it the column names of features (here, the index values) that need to be dropped from the data. Rember that 'isnull().sum()' returns a series of the number of missing value, with the original dataframe column names used as index values.\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e397133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create the list of missing values\n",
    "missing_values = ((X_train.isnull().sum())/X_train.shape[0]*100)\n",
    "\n",
    "# Filter for >30% and sort it in descending order\n",
    "missing_values = missing_values[missing_values>30]\n",
    "missing_values.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of names to drop from the dataframe based on missing_values\n",
    "to_drop = missing_values.index.values\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678fab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop and check the new dataframe\n",
    "X_train = X_train.drop(to_drop, axis=1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecec07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again for missing values > 0: none should be above 30%\n",
    "missing_values = ((X_train.isnull().sum())/X_train.shape[0]*100)\n",
    "missing_values = missing_values[missing_values>0]\n",
    "missing_values.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0ec76c",
   "metadata": {},
   "source": [
    "## `RoadWidthM`, `PowerlineD` and `Trees`\n",
    "üõÇ Check for missing values in `RoadWidthM`, `PowerlineD` and `Trees` and deal with them appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb9555",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary> üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è Look at the datatype of <code>PowerlineD</code> and the distribution of the data using the <code>.unique()</code> method. Although <code>PowerlineD</code> is a numeric value, it clearly only has discrete distribution: what would be a logical value to impute? The same applies to <code>Trees</code> and <code>RoadWidthM</code> but for a different reason: they are a continuous variable but there is clearly one value that dominates the distribution: it makes sense to assume that the `nan` represent this most frequent value. So you can impute both of these variables at the same time.\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba91a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'# Missing RoadWidthM:{X_train.RoadWidthM.isnull().sum()}')\n",
    "print(f'# Missing PowerlineD:{X_train.PowerlineD.isnull().sum()}')\n",
    "print(f'# Missing Trees:{X_train.Trees.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.PowerlineD.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb73e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.PowerlineD.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.PowerlineD.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937964fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.Trees.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0cf668",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.RoadWidthM.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf60f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(3,1,figsize=(8,12))\n",
    "\n",
    "for ax, feature in zip(axes.flatten(), ['PowerlineD','RoadWidthM', 'Trees']):\n",
    "    X_train[feature].plot(kind='hist',ax=ax)\n",
    "    ax.set_title(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2680945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "cw_imputer = SimpleImputer(strategy='most_frequent').fit(X_train[['PowerlineD','Trees','RoadWidthM']])\n",
    "X_train[['PowerlineD','Trees','RoadWidthM']]= cw_imputer.transform(X_train[['PowerlineD','Trees','RoadWidthM']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee3579",
   "metadata": {},
   "source": [
    "## `Locality` and `EVCNotes`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b364e88",
   "metadata": {},
   "source": [
    "üõÇ Check for missing values in `Locality` and `EVCNotes` for missing values and deal with them appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1856e280",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è Clearly <code>Locality</code> refers to the name of the county or region where the data comes from. We could impute the most frequent locality, but this would induce some errors. In this case, the best strategy is simply to replace the <code>nan</code> by something meaningful such as 'not known'. <code>EVCNotes</code> is somewhat similar: the <code>nan</code> values indicate that no notes exist, so we should replace them by 'no notes'.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'# Missing Locality:{X_train.Locality.isnull().sum()}')\n",
    "print(f'# Missing EVCNotes:{X_train.EVCNotes.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb62746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dtypes and values\n",
    "X_train.Locality.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de831e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.Locality.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.EVCNotes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d91f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.EVCNotes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace wiht appropriate labels\n",
    "X_train.loc[X_train.Locality.isnull(),'Locality'] = 'not known'\n",
    "X_train.loc[X_train.EVCNotes.isnull(),'EVCNotes'] = 'no notes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f73eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that there are no more null values\n",
    "X_train.Locality.isnull().sum() + X_train.EVCNotes.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d15a656",
   "metadata": {},
   "source": [
    "## `SoilType` and `LandFormLS`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a80b07b",
   "metadata": {},
   "source": [
    "üõÇ Check for missing values in `SoilType` and `LandFormLS` and deal with them appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8478b3b",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è These two are tricky. They both are string values, and they both have two classes that are very common. On a real project, a good data scientist will study what those codes means <a href=\"http://vro.agriculture.vic.gov.au/dpi/vro/vrosite.nsf/pages/landform_land_systems_rees/$FILE/TECH_56%20ch6.pdf\"> by refering to the government publication</a>. In an ideal world we would explore different strategies for imputation (we will see this later in the course). However here we need to decide based on little evidence. Because we have no information, and because there is not a clear majority in either soil or landform classes, the best is to impute 'SoilTypeNA' and 'LandFormLSNA' as as a new class.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b82aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'# Missing SoilType:{X_train.SoilType.isnull().sum()}')\n",
    "print(f'# Missing LandFormLS:{X_train.LandFormLS.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad64f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SoilType column\n",
    "X_train.SoilType.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.SoilType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e39887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check the LandFormLS column\n",
    "X_train.LandFormLS.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d18eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.LandFormLS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b367c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't easily justify replacing by most frequent - instead create new NA values\n",
    "\n",
    "X_train.loc[X_train.LandFormLS.isnull(),'LandFormLS'] = 'LandFormLSNA'\n",
    "X_train.loc[X_train.SoilType.isnull(),'SoilType'] = 'SoilTypeNA'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1ee507",
   "metadata": {},
   "source": [
    "## `CanopyCont`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918bbdae",
   "metadata": {},
   "source": [
    "üõÇ Check for missing values in `CanopyCont` and deal with them appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5cd3b",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è If you do a <code>value_counts()</code> on <code>CanopyCont</code> you will see that this consists of 4 numerical variables, and 5 categorical variables. It is clear that this column has two different encoding for the same concept: how continuous is the canopy? The easiest is to transform this into a numerical column by doing the following replacements: 'none'=0, 'sparse'=1, 'patchy'=2, 'continous' or 'c' = 3. You probably want to use a python dictionary and an <code>apply()</code> function to do that, and remember to cast your values to an <code>int</code> or a <code>float</code>!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a64f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'# Missing CanopyCont:{X_train.CanopyCont.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a916f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution of values in CanopyCont\n",
    "X_train.CanopyCont.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bf5295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionaty to replace strings by int:\n",
    "dic = {'none':0,\n",
    "      'sparse':1,\n",
    "      'patchy':2,\n",
    "      'continuous':3,\n",
    "      'c':3}\n",
    "\n",
    "# Use an apply lambda function to replace the value. Cast to int, and return default value as 'x'\n",
    "X_train['CanopyCont'] = X_train.CanopyCont.apply(lambda x:int(dic.get(x, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefb2a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that it all worked well\n",
    "X_train.CanopyCont.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147eb0fe",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d280f33c",
   "metadata": {},
   "source": [
    "üëá Investigate the numerical features for outliers and distribution, and apply the solutions below accordingly:\n",
    "- Robust Scale\n",
    "- Standard Scale\n",
    "\n",
    "Replace the original columns by the transformed values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74417cfe",
   "metadata": {},
   "source": [
    "## `WidthVarie` , & `Powerline`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b60bb44",
   "metadata": {},
   "source": [
    "‚öñÔ∏è Scale `WidthVarie` and `Powerline` using the most appropriate scaler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc66849",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è <code>WidthVarie</code>, & <code>Powerline</code> are clearly binary variable ([0,1]). They should not be scaled, but rater can optionally be encoded using a <code>CategoricalEncoder</code>. Simply leave them as they are.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b111cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.Powerline.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.WidthVarie.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7696de",
   "metadata": {},
   "source": [
    "## All other numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d438e",
   "metadata": {},
   "source": [
    "‚öñÔ∏è How would you scale all of the other variables? Save a list of the numerical column names (minus `WidthVarie` and `Powerline`, see above) in a variable called `numerical_columns`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb32fd0",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è All other variables are continous, but their distribution is non-gaussian. We can use a RobustScaler() here. The first task is to identify the columns with a dtype of either 'float64' or 'int64': you can do this programmatically to avoid having to type a long list of features!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224cb2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check for float and int variables\n",
    "floatv = X_train.dtypes[X_train.dtypes=='float64'].index.values\n",
    "intv = X_train.dtypes[X_train.dtypes=='int64'].index.values\n",
    "\n",
    "# vstack the two data types into one numpy array\n",
    "numerical_columns = np.hstack([floatv,intv])\n",
    "\n",
    "#Transform numerical_colums to a list to more easily remove the two features\n",
    "numerical_columns = numerical_columns.tolist()\n",
    "numerical_columns.remove('Powerline')\n",
    "numerical_columns.remove('WidthVarie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce8087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a few plot to check the distribution of the data\n",
    "fig, axes = plt.subplots(len(numerical_columns),figsize=(8, 120))\n",
    "\n",
    "for ax, feature in zip(axes, numerical_columns):\n",
    "    X_train[feature].plot(kind='hist', ax=ax)\n",
    "    ax.set_title(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b963a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import  RobustScaler\n",
    "\n",
    "X_train[numerical_columns] = RobustScaler().fit_transform(X_train[numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe81ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ffb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b6ec6",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e75806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('missing_values',\n",
    "                         dataset = X_train)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3027ceb",
   "metadata": {},
   "source": [
    "### Testing your scaling\n",
    "Test your code below for scaling before proceeding to ensure all worked well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec9c1dd",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4dba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('scaling',\n",
    "                         dataset = X_train,\n",
    "                         features = numerical_columns\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba7a26d",
   "metadata": {},
   "source": [
    "# Part III: Encoding and Modelling\n",
    "\n",
    "All that is left to do now is deal with categorical data, and then use this to build a simple model.\n",
    "\n",
    "# Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6c7ce",
   "metadata": {},
   "source": [
    "üëá Investigate the non-numerical features that require encoding, and apply 'One hot encoding'. To ensure that we do not end up with an explosion of feature, we will retain only categorical features with <15 unique values for encoding. \n",
    "\n",
    "So your task is the following:\n",
    "\n",
    "1. Identify programmatically all of the categorical features that have <15 unique categories and require 'One Hot encoding'\n",
    "2. In the dataframe, replace the original features by their encoded version(s). Make sure to drop the original features, as well as the features with >15 unique categories from `X_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's obtain the ohe_features of dtype 'object'\n",
    "ohe_features = X_train.dtypes[X_train.dtypes==object].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f5937",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73859fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can loop through each feature to ensure their nunique\n",
    "small_ohe = []\n",
    "\n",
    "for feature in ohe_features:\n",
    "    if (X_train[feature].nunique()<15):\n",
    "        small_ohe.append(feature)\n",
    "small_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08199af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the small_ohe columns (only 2) we can encode them:\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder().fit(X_train[small_ohe])\n",
    "\n",
    "# horizontal stack of the two arrays\n",
    "columns = np.hstack(ohe.categories_)\n",
    "\n",
    "# Create an array of one hot encoded values\n",
    "ohe_df = pd.DataFrame(ohe.transform(X_train[small_ohe]).toarray(), columns=columns)\n",
    "\n",
    "# Reset indexes so both dfs have the same (important: drop=True to avoid keeping index as a column)\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "ohe_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Join the one hot encoded values to the original dataframe\n",
    "X_train = X_train.join(ohe_df)\n",
    "\n",
    "# Drop ALL original categorical variables from the dataframe and check it visually\n",
    "X_train.drop(ohe_features, axis=1, inplace=True)\n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf78b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have the new category in the columns\n",
    "X_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05363705",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ce75d",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a85bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('encoding',\n",
    "                         dataset = X_train)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c94e65",
   "metadata": {},
   "source": [
    "# Base Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a155b36f",
   "metadata": {},
   "source": [
    "All we need now is to cross validate  a Linear regression model with our `X_train` and `y_train` using `cv=5`. Save its score under variable name `base_model_score`. However, if you do this you will see that we obtain a very low `r2`. This is because not all of the features we have selected are useful - we will talk more about this in a couple of days. So instead, train your model using only the top features that have a correlation with your `y_train` > 0.05.  <details><summary>üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è If you are unsure how to do this, check the documentation for the `corr()` function in pandas. Also, you will need to add group the `y_train` and the `X_train` in the same pandas object to do that.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca28c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = X_train.copy()\n",
    "corr['target']=y_train.copy()\n",
    "\n",
    "corr = corr.corr().abs().target.sort_values(ascending=False)\n",
    "\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a07416",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = corr[corr>.05][1:].index.values\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108eca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cross_val = cross_validate(estimator=LinearRegression(),X=X_train[features], y=y_train, cv=5, scoring='r2',)\n",
    "\n",
    "base_model_score = cross_val['test_score'].mean()\n",
    "base_model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174131ee",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e523106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('base_model',\n",
    "                         score = base_model_score\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05d7e1d",
   "metadata": {},
   "source": [
    "# üèÅ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
