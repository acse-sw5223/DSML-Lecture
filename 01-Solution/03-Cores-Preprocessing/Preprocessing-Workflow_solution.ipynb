{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26be52c8",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-cL5eOpEsbuIEkvwW2KnpXC12-PAbamr\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2678f0",
   "metadata": {},
   "source": [
    "# Preprocessing Core Data from IODP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443930b3",
   "metadata": {},
   "source": [
    "üéØ This exercise will take you through the preprocessing workflow with only a little less help then before. Step by step, feature by feature, you will investigate the dataset and take preprocessing decisions accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0538ba",
   "metadata": {},
   "source": [
    "# Introduction to Core Data\n",
    "\n",
    "![glomar](http://deepseadrilling.org/images/challenger1.jpg)<br>\n",
    "One of the objectives of this module is to familiarize yourself with new data types. Geologist learn a lot about the history of the Earth by studying core material, i.e. cylindrical pieces of rock recovered from the subsurface by a drill string. Drilling for cores can be performed on land, or at sea from a drilling vessel. One famous drilling vessel is the R/V JOIDES Resolution. I also used to work as a staff scientist on the JOIDES Resolution (aka \"JR\") before joining Imperial College, so I know it well. The JR is operated by the <a href=\"https://www.iodp.org/\">International Ocean Discovery Program (IODP, 2003-today)</a>, which was preceded by the <a href=\"http://www-odp.tamu.edu/\">Ocean Drilling Program (ODP, 1985-2003)</a> and the <a href=\"http://deepseadrilling.org/about.htm\">Deep Sea Drilling Project (DSDP, 1966-1985)</a>. The various version of the ocean drilling program thus represents one of the major investment in Earth Science over the last 55 years, and has acquired a treasure trove of core and logging data. We will take full advantage of this here.<br>\n",
    "\n",
    "![cores](https://www.iodp.org/images/right_side_random_images/357_mircobio_carolcotterill.jpg)<br>\n",
    "\n",
    "Because we will be using ODP/IODP data extensively in this course it is worth defining what the different terms represent.\n",
    "\n",
    "'Leg' refers to the particular expedition the drill ship was on. In more recent IODP cruises, this is referred to as the 'Expedition' or 'Exp'. Think of a leg as a 6-9 week-long cruise with a single crew, addressing a single research objective. Leg or Expedition are represented by a sequential <code>Int</code>.\n",
    "\n",
    "'Site' means the name of the general area that was drilled, which is also a sequential number. At each site, the vessel can drill multiple wells, known a 'Hole' and often represented with the letter 'H'. The wells are represented by an ordered <code>str</code>: 'A', 'B', 'C', etc..\n",
    "\n",
    "Although a drilling vessel aims to recover a continuous section of rock, for practical reasons this is not possible. Thus, pipes are added to the drill string as coring down continues, and a length of rock nominally the length of the drilling pipe is recovered. The pipes on the JR are 9.8 meters, and so we recover cores of 9.8 meters. Cores are denoted with a unique number ('Cor'), followed by the tool used for coring ('T'), which is a 1 character designation of the drilling technique used (more about this later in the course). 'Sc' designates the section: each 9.8 meters-long core is cut into 1.5 meters sections for easy handling, with the addition of the core catcher 'CC' which represents material not in the core liner (in oder words, we are not sure where this material comes from - the core itself, the side of the hole, or the seafloor). For each sample from the ocean drilling program, a 'Top(cm)' is defined and represents the cm down from the top of the section where the sample is located.\n",
    "\n",
    "Thus, an ODP/IODP sample can be fully spatially resolved by the combination of the parameters we have described above (and that are always supplied as part of the dataset as a unique identifyer). For instance, an example of a sample would be ODP Sample **198-1207-A-2-H-2-65**. This means that this particular sample comes from ODP Leg 198, Site 1207, Hole A (or 'well A'), core 2 drilling with a piston corer ('H'), section 2, 65 cm down section. With this information the 'Depth (mbsf)' which is the depths below seafloor of any sample can be calculated.\n",
    "\n",
    "Multiple sets of properties can be measured on a core sample, either onboard the ship or postcruise, once the scientists are back onshore. All of this information can be precisely correlated and used by data scientists like yourselves.\n",
    "\n",
    "![cores](https://iodp.tamu.edu/database/wholecore.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec5759",
   "metadata": {},
   "source": [
    "üëá Run the code below to load the dataset and features you will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581778ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='1l-8v0bV_qY8OSoc1QwcflQyKHKUPHK6y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e96ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('raw_data/core_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c62d18f",
   "metadata": {},
   "source": [
    "üëâ Take the time to do a preliminary investigation of the features. Notice that you have a column names 'NGR total counts (cps)': this represents the natural gamma ray reading of the core, and there is a complete explanation available [here](http://www-odp.tamu.edu/publications/tnotes/tn26/CHAP5.PDF). You will also see columns named 'L*', 'a*', and 'b*': this refers to the reflectance data from the core, expressed in the CIELAB color space. You can learn much more about this by reading [this techincal note](http://www-odp.tamu.edu/publications/tnotes/tn26/CHAP7.PDF). Reading the technical notes is not essential to understanding this exercise, so consider this optional but good to have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d96846e",
   "metadata": {},
   "source": [
    "# Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423be57d",
   "metadata": {},
   "source": [
    "‚ÑπÔ∏è Duplicates in datasets can cause data leakage. It is important to locate and remove any meaningless duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c9e38",
   "metadata": {},
   "source": [
    "‚ùì How many duplicated rows are there in the dataset? Save your answer under variable name `duplicate_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6b066",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_count = data.duplicated().sum()\n",
    "duplicate_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4fd9b",
   "metadata": {},
   "source": [
    "üëá Remove the duplicates from the dataset. Overwite the dataframe `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f44930",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0f3289",
   "metadata": {},
   "source": [
    "# Splitting your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e2fa30",
   "metadata": {},
   "source": [
    "Now split your dataset into a `train_set` (80%) and a `test_set` (20%). Use `42` as your `random_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92000204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(data, train_size=.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a57940",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec90a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('duplicates',\n",
    "                         duplicates = duplicate_count,\n",
    "                         dataset = data\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951602ec",
   "metadata": {},
   "source": [
    "# Missing data\n",
    "From now on, use <span style=\"color:red\">only your `train_set`</span>, ***NOT*** your `data` or your `test_set`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946c6954",
   "metadata": {},
   "source": [
    "üëá Print out the percentage of missing values for all columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be0878",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.isnull().sum()/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb8b651",
   "metadata": {},
   "source": [
    "## `Lithology`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e8d07",
   "metadata": {},
   "source": [
    "üëá Investigate the missing values in `Lithology`. Then, chose one of the following solutions:\n",
    "\n",
    "1. Drop the column entirely\n",
    "2. Impute the column median using Sklearn's `SimpleImputer`\n",
    "3. Preserve the NaNs and replace by actual meaning\n",
    "\n",
    "Make changes effective in the dataframe `data`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e9367",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint</summary>\n",
    "    ‚ÑπÔ∏è <code>Lithology</code> has a lot of missing values. The description does not touch on what they represent. As such, it is better not to make any assumptions and to drop the column entirely.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96d3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.drop('Lithology', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcf9bcf",
   "metadata": {},
   "source": [
    "## `NGR total counts (cps)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f6f2e7",
   "metadata": {},
   "source": [
    "üëá Investigate the missing values in `NGR total counts (cps)`. Then, chose one of the following solutions:\n",
    "\n",
    "1. Drop the column entirely\n",
    "2. Impute the column median using Sklearn's `SimpleImputer`\n",
    "3. Preserve the NaNs and replace by actual meaning\n",
    "\n",
    "Make changes effective in the dataframe `data`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a360271",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint</summary>\n",
    "‚ÑπÔ∏è <code>NGR total counts (cps)</code> has a few missing values that can be imputed by the median value.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "NGRimputer = SimpleImputer(strategy='median')\n",
    "\n",
    "train_set['NGR total counts (cps)'] = NGRimputer.fit_transform(train_set[['NGR total counts (cps)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c75dff",
   "metadata": {},
   "source": [
    "üëá When you are done, print out the percentage of missing values for the entire dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236fcb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.isnull().sum().sum()/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2fbedd",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Be careful: not all missing values are represented `np.nans`, and python's `isnull()` only detects `np.nans` ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c8867",
   "metadata": {},
   "source": [
    "## `Reflectance L*, a* and b*`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e923a",
   "metadata": {},
   "source": [
    "üëá Investigate the missing values in `Reflectance L*, a*, b*`. Then, chose one of the following solutions:\n",
    "\n",
    "1. Drop the column entirely\n",
    "2. Impute the column mean using Sklearn's `SimpleImputer`\n",
    "3. Preserve the NaNs and replace by actual meaning\n",
    "\n",
    "Make changes effective in the dataframe `data`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5985ba3d",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint</summary>\n",
    "‚ÑπÔ∏è <code>Reflectance L*, a*, b*</code> have few missing values that can be imputed by the mean value. You can do this with a single imputer.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d284f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "RefImputer = SimpleImputer(strategy='mean')\n",
    "train_set[['Reflectance L*', 'Reflectance a*','Reflectance b*']]=RefImputer.fit_transform(train_set[['Reflectance L*','Reflectance a*','Reflectance b*']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028b188a",
   "metadata": {},
   "source": [
    "## `Type`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec6c4a7",
   "metadata": {},
   "source": [
    "üëá Investigate the missing values in `Type`. Then, chose one of the following solutions:\n",
    "\n",
    "1. Drop the column entirely\n",
    "2. Impute the column median\n",
    "3. Preserve the NaNs and replace by actual meaning\n",
    "\n",
    "Make changes effective in the dataframe `data`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d3863c",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint</summary>\n",
    "‚ÑπÔ∏è <code>Type</code> represents the type of the coring apparatus used. It is a string, not a number. Only a few values are missing, and you don't want to loose important information by dropping the rows: instead you could assume that the type can be replaced by the most frequent value without loosing much information. Check the <code>SimpleImputer</code> documentation to see how to do that. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383045c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "typeImputer = SimpleImputer(strategy='most_frequent')\n",
    "train_set[['Type']] = typeImputer.fit_transform(train_set[['Type']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c460b1",
   "metadata": {},
   "source": [
    "## `Depth CSF-A (m)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a49a31f",
   "metadata": {},
   "source": [
    "üëá Investigate the missing values in `Depth CSF-A (m)`. Remember that this represents the depth of the sample below surface, a critical piece of information that cannot easily be supplemented by other data. With this in mind, chose one of the following solutions:\n",
    "\n",
    "1. Drop the rows entirely\n",
    "2. Impute the column median\n",
    "3. Preserve the NaNs and replace by actual meaning\n",
    "\n",
    "Make changes effective in the dataframe `data`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa449c",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint</summary>\n",
    "‚ÑπÔ∏è <code>Depth CSF-A (m)</code> is only missing in a few rows. Because we cannot supplement it easily and because a 'mean' depth for a sample has no real meaning, a good strategy here would be to drop the rows where this value is NaN. Look at the pandas documentation to see how to drop specific rows: think also of how you would find the index of the rows that need dropping.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07536317",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = train_set[train_set['Depth CSF-A (m)'].isnull() == True].index\n",
    "\n",
    "train_set.drop(index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78193ce7",
   "metadata": {},
   "source": [
    "### Check to see if you still have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b00c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b1e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebf05af",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8801bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('missing_values',\n",
    "                         dataset = train_set\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab5e436",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "In order to investigate features and, we recommend that you plot a histogram and a box plot for each one of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41838cb9",
   "metadata": {},
   "source": [
    "##  `Reflectance L*` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c28b83",
   "metadata": {},
   "source": [
    "üëá Investigate `Reflectance L*` for distribution and outliers. Then, choose the most appropriate scaling technique. Either:\n",
    "\n",
    "1. Standard Scale\n",
    "2. Robust Scale\n",
    "3. MinMax Scale\n",
    "\n",
    "Replace the original columns by the transformed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60348ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(train_set['Reflectance L*']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7c0a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[['Reflectance L*']].boxplot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5354b526",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint</summary>\n",
    "‚ÑπÔ∏è Since <code>Reflectance L*</code> does  seem to have a normal distribution, so we can use the <code>StandardScaler</code> to scale. Note that it would not be incorrect to use <code>MinMax()</code> or <code>RobustScaler()</code>.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb6d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_set['Reflectance L*'] = StandardScaler().fit_transform(train_set[['Reflectance L*']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0e08e",
   "metadata": {},
   "source": [
    "## `Refectance a*` & `Reflectance b*`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d6ae8",
   "metadata": {},
   "source": [
    "üëá Investigate `Reflectance a*` & `Reflectance b*`. Then, chose one of the following scaling techniques:\n",
    "\n",
    "1. MinMax Scale\n",
    "2. Standard Scale\n",
    "3. Robust Scale\n",
    "\n",
    "Replace the original columns by the transformed values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0974ff4",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint</summary>\n",
    "‚ÑπÔ∏è <code>Reflectance a*</code> and <code>Reflectance b*</code> are both normally distributed but with some outliers: we could use the <code>RobustScaler()</code> here.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(train_set['Reflectance a*']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ed713",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[['Reflectance a*']].boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af9d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(train_set['Reflectance b*']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a500993",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[['Reflectance b*']].boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096bf113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "abScaler = RobustScaler()\n",
    "train_set[['Reflectance a*','Reflectance b*']] = abScaler.fit_transform(train_set[['Reflectance a*','Reflectance b*']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8185ac60",
   "metadata": {},
   "source": [
    "## `Depth CSF-A (m)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7b4722",
   "metadata": {},
   "source": [
    "üëá Investigate `Depth CSF-A (m)` for distribution and outliers. Then, choose the most appropriate scaling technique. Either:\n",
    "\n",
    "1. Standard Scale\n",
    "2. Robust Scale\n",
    "3. MinMax Scale\n",
    "\n",
    "Replace the original columns by the transformed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b4009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(train_set['Depth CSF-A (m)']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691569c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[['Depth CSF-A (m)']].boxplot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8da2dc1",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint</summary>\n",
    "‚ÑπÔ∏è <code>Depth CSF-A (m)</code> does not show a normal distribution, it is better to <code>MinMax()</code> scale to ensure all results are between 0 and 1.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7fa239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "depthScaler = MinMaxScaler()\n",
    "\n",
    "train_set['Depth CSF-A (m)'] = depthScaler.fit_transform(train_set[['Depth CSF-A (m)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ebd30",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ce799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('scaling',\n",
    "                         dataset = train_set\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733b4031",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8293f9",
   "metadata": {},
   "source": [
    "## `Type`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26300899",
   "metadata": {},
   "source": [
    "üëá Investigate `Type` and chose one of the following encoding techniques accordingly:\n",
    "- Ordinal encoding\n",
    "- One-Hot encoding\n",
    "\n",
    "Add the encoding to the dataframe as new colum(s), and remove the original column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c4e20",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint</summary>\n",
    "‚ÑπÔ∏è `Type` is a multicategorical feature that must be One hot encoded: there is no explicit ordinal value to the drill bit type.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5323e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what unique value of types exist:\n",
    "train_set.Type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccaaa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False).fit(train_set[['Type']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b8807",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e5d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['H'],train_set['X'],train_set['R']= ohe.transform(train_set[['Type']]).T # We need to transpose the data into columns\n",
    "\n",
    "train_set.drop('Type', axis=1, inplace = True)\n",
    "\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92907ff",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebbab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('encoding', dataset = train_set, new_features = ['H', 'X', 'R'])\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba97c33",
   "metadata": {},
   "source": [
    "# üèÅ"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
