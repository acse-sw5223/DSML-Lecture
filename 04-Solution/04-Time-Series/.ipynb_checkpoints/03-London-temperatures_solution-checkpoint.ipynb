{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d2b2fd",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-hPP-XPm9_5M3orUgmompcVleQ5xvPST\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a087d",
   "metadata": {},
   "source": [
    "# Time Series Forecasting: London Monthly Temperatures from 1750 to 2013\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1H3N3tkWYoetiK0zra4kAevwLzVD2FyWV\" style=\"Width:600px\">\n",
    "\n",
    "We will use the **City Temperatures** dataset once more, but this time taking advantage of the long-term observations recorded in cities. In particular, we will look at <strong style=\"color:teal\">London</strong> as we have a regular, monhly temperature record for this city dating back to 1750. So, over 250 years of measurements every month!\n",
    "\n",
    "The goal in this exercise is to predict the temperature of the next month based on the record of the 11 previous months. This will teach you some useful libraries and practices in machine learning time series prediction. Predicting a value based on past (or 'lagged') values of itself is also called \"endogenic prediction\", as the variable you use for the prediction is the same as the one you make the prediction on (i.e. it is an endogenic variable as opposed to an exogenic variable).\n",
    "\n",
    "## Open the data\n",
    "\n",
    "Go ahead, and create a dataframe called `data` based loading the `london_historical_tempratures.csv` file. Check for missing values, and also look at the data types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18391fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='1w62Fyikdqh8aOquDB8ihQkqtECJU7NbL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d973e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('raw_data/london_historical_temperatures.csv')\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54702ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a4c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1571050",
   "metadata": {},
   "source": [
    "# Datetime objects\n",
    "\n",
    "If you looked at the data types, you will have seen that the `dt` variable (which stands for `datetime`) is in fact in string format. If you tried to plot `AverageTemperature` versus `dt`, you would obtain an error because you cannot plot strings against floats.\n",
    "\n",
    "So we need to transform the string version of the date into a format that `Pandas` can interpret as a `datetime` object. Luckily, there is a convenient functions for that: `pd.to_datetime`. Explore this function (read the doc!), and convert the original `dt` in your dataframe into a `datetime` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e32454",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dt=pd.to_datetime(data.dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895654f",
   "metadata": {},
   "source": [
    "## Looking at the difference between datetimes\n",
    "\n",
    "Now that we have proper `datetime` object, we can do interesting things, like for instance looking at the number of days between each observations. This is important, because we will not use the `datetime` object in our prediction: our assumption is thus that the temperature measurements are equally spaced in time. If there was a gap, this would be an issue.\n",
    "\n",
    "`Pandas` has a very useful function called `diff()`, which standas for differentiation. This essentially takes the value from the previous row (or $n^{th}$ previous row if you give an `int` as function argument) and subtract it from the current row. We can sometimes use this to differentiate a timeseries, and thus render it stationary. But here, this is not our goal.\n",
    "\n",
    "Instead, do the following:\n",
    "\n",
    "* Create a new `pd.Series` called `diff`, which is the difference between the `AverageTemperature` and its previous value\n",
    "* Find the maximum value of `diff`: does it suggest you have no gaps in your record?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = data.dt.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a7f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb545456",
   "metadata": {},
   "source": [
    "## Dealing with time gaps\n",
    "\n",
    "You should have found above that there is a 183 days gap somewhere in the record. So, effectively, half a year is missing somewhere. This is an issue, because our modelling will assume no time gaps. We need to find where the gap is. You can do this by using `argmax` on the `diff` series, and this will return the index of the value immediately after the gap. Look at the data starting immediately before the gap to convince you that it is real.\n",
    "\n",
    "Then, simply drop the rows before the gap, and save the new values as `data`. To make sure you no longer have gaps in your record, redo a `diff()` and check for the maximum value: it should be 31 days, which is the longest period of a calendar month. If it isn't, repeat the same process again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dc3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ac7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[21:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eb8472",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dt[22:].diff().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8af91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[22:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd17c6",
   "metadata": {},
   "source": [
    "### ☑️ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c64579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('data_selection',\n",
    "                         data = data\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfd0cc9",
   "metadata": {},
   "source": [
    "# Time Series data preparation\n",
    "\n",
    "You now have a continuous and evenly sampled dataset from 1752 to 2013! \n",
    "\n",
    "First, have a look at the data by plotting the `dt` versus `AverageTemperature`. Then, once you have a sense of what you are dealine with, move on to decompose your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711f5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(25, 5))\n",
    "\n",
    "ax.plot(data.dt, data.AverageTemperature);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f91de2",
   "metadata": {},
   "source": [
    "## Time Series Decomposition\n",
    "\n",
    "A times series can be viewed as being composed of three different components:\n",
    "1. A trend, or long-term change in the weighted mean of the data\n",
    "2. A seasonal trend, or long-term cyclicity\n",
    "3. The residuals, i.e. the difference between the data and the sum of the trend and the season\n",
    "\n",
    "The trend and the seasonal trend are non-stationary, and in `machine learning` we typically remove them and predict the `residuals`.\n",
    "\n",
    "But how do we obtain the `trend`, `seasonal component`, and `residual` values? There are many ways, but one of the most convenient tool is to use `statsmodel` and it suites of time series analysis (`tsa`) tools. As a note, time series analysis is a complex subject, and in this module we just scratch the surface of what can be done with these tools.\n",
    "\n",
    "The tool we are most interested in right now is a function called `seasonal_decompose` that resides in the `statsmodels.tsa.seasonal` module. The documentation <a href=\"https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html\"> can be found here</a>. The function takes a series as input, as well as a model (this can be either `additive` or `multiplicative`: either the trend, season and residual need to be added together to reconstruct the original time series, or multiplied together) as well as the period for the season. It returns a panda-like object that containts three useful values for us: the `trend`, `seasonal` and `resid` values.\n",
    "\n",
    "Go ahead and use the `seasonal_decompose` function using an additive model and a period corresponding to a calendar year. Keep in mind that the `period` is entered in the unit of observation. Save the return value of the function in a variable (I called it `results`), and then create three new features in `data` that corresponds to the equivalent values in `results`: `trend`, `season`, and `residuals`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8799cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "results = seasonal_decompose(data.AverageTemperature, model='additive',period=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcaee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['trend'] = results.trend\n",
    "data['season'] = results.seasonal\n",
    "data['residuals'] = results.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf798d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d221812",
   "metadata": {},
   "source": [
    "# Plotting the different component of temperature in London\n",
    "\n",
    "👉🏽 First, plot the `data.dt` versus `data.trend`: can you spot the clear trend towards warming in London, and when it started? Save the date you think warming in London started in a variable named `first_warming`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 5))\n",
    "\n",
    "ax.plot(data.dt,data.trend);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec52e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_warming = 1900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aadeb36",
   "metadata": {},
   "source": [
    "👉🏽 Now, let's look at the seasonality. Plot the first 3 years of the record (`dt` vs `season`) and see if you can spot the difference between winter and summer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f42b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 5))\n",
    "\n",
    "ax.plot(data.dt[:36],data.season[:36]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8447a7d",
   "metadata": {},
   "source": [
    "👉🏽 Then, plot the `residuals` and see if you think that they are stationary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57673d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(25, 5))\n",
    "\n",
    "ax.plot(data.dt,data.residuals);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82294d8a",
   "metadata": {},
   "source": [
    "👉🏽 And finally, convince youself that you can reconstruct the original dataset by adding the `trend`, `seasons` ans `residuals` and plotting their sum. Does it look like the orinal series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2447aec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(25, 5))\n",
    "\n",
    "ax.plot(data.dt,data.trend+data.season+data.residuals);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44548380",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.residuals.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ce5c0",
   "metadata": {},
   "source": [
    "### ☑️ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0545cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('decomposition',\n",
    "                         data = data\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9829d9",
   "metadata": {},
   "source": [
    "# Ensuring stationarity\n",
    "\n",
    "The modelling approach we will use today relies on `stationarity`. So, we need to ensure that our data is stationary. Visually, the resisuals should look ok in your plot above. But is there a statistical way to ensure that this is the case?\n",
    "\n",
    "🧞‍♀️ The [`Augmented Dick Fuller test`](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test) was developpd precisely for this purpose. The augmented Dick Fuller test is a statististical test that was created for financial times series, and the time series can be deemed stationary if you can reject at a 95% confidence level the null hypothesis of non-stationarity. We won't go into the details here, but if you are interested you can read more about this test by following the link I gave you.   \n",
    "\n",
    "The easiest way to apply the Augmented Dick Fuller test is to use the <a hreaf=\"https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.adfuller.html\">statsmodel.tsa.stattools.adfuller</a> function. Check the documentation to see how to use the tool, and especially how to read the results and find the p-value outputted by this test: this will be the crucial parameter. Remember, because we want to reject non-stationarity at 95% CL, if the p-value of our test is <0.05 then we can be confident that the series is stationary.\n",
    "\n",
    "## Is our original time series stationary?\n",
    "\n",
    "To convince yourself that the test works, first try it on your original time series (`AverageTemperature`). Is the p-value < 0.05?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aec13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "adfuller(pd.DataFrame(data.AverageTemperature))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5daf762",
   "metadata": {},
   "source": [
    "<details><summary>🧞‍♂️ Observation</summary><br>\n",
    "    Given that we know we have a long-term trend and a seasonal component in the <code>AverageTemperature</code>, we should not expect non-stationarity. And in fact, the p-value returned by the test is very high (it corresponds to the second value return by the test)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6154e49a",
   "metadata": {},
   "source": [
    "## Is our `residuals` stationary?\n",
    "\n",
    "Now do a second test, but this time on our `data.residuals`: is the p-value < 0.05?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "adfuller(pd.DataFrame(data.residuals).dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50588869",
   "metadata": {},
   "source": [
    "<details><summary>🧞‍♂️ Observation</summary><br>\n",
    "    This time, the p-value is very low, in fact for me it is even zero.<br><br>\n",
    "    <strong>💪 We have achieved stationarity, and can now model our time series using machine learning!</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7926384",
   "metadata": {},
   "source": [
    "# Preparing the data for machine learning\n",
    "\n",
    "We will use the `residuals` as both our features `X` and our target `y`. But in order to be able to do that, we need to prepare the data in two ways:\n",
    "\n",
    "1. We need to create the matrix of past observations `X`\n",
    "2. We need to train-test-split our data in a way to avoids data leakage\n",
    "\n",
    "Let's do it!\n",
    "\n",
    "## Creating our `features`\n",
    "\n",
    "Our features will be the 11 months of observations (in time series analysis, these are know as `lags`) before the actual target (our `residuals` value will be the `y`). `Pandas` has a very convenient function called `shift()`: it will shift a column by `n` lags (`n` can be provided as an input to `shift`, the default value is 1). So, if we want to create a columns with a lag of 1, we can call `shift()` on the original column. For a lag of two, we call `shift(2)`, etc...\n",
    "\n",
    "Create 11 new columns in `data` representing the shifted values of `residuals` by 1 to 11 lags\n",
    "\n",
    "<details><summary>📞 Tip</summary><br>\n",
    "    This can be done in 2 easy lines of python code if you use a loop</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 12):\n",
    "    data.loc[:,f'{i}-months-lag'] = data.residuals.shift(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f341d9f",
   "metadata": {},
   "source": [
    "### Problem with `nan` values\n",
    "\n",
    "Look at `data` now: you should see a whole lot of `nan` values. This is because **(a)** our residual has `nan` values at the start that result from how it was computed, and **(b)** we cannot have an $11^th$ (or $10^th$, $9^th$ , etc...) lag at the begining of our time series as we don't have enough past observations.\n",
    "\n",
    "In theory, we could impute these data, but this would add noise to our data. Instead, create a new dataframe called `timeseries` that is the same as `data` but with all the `nan` dropped!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1fc64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae54d3",
   "metadata": {},
   "source": [
    "## Splitting the data into a `train_set` and `test_set`\n",
    "\n",
    "Rember that for `geospatial` and `time series` data we **DO NOT** want to do a random split. Otherwise, we will have data leakage. So, we will keep 70% of the data for training, and 30% of the data for testing. The way you will split the data is the following:\n",
    "\n",
    "* Find the index (save it as `split_idx`) in `timeseries` where 70% of the data lies between the begining of the dataset (1752) and `split_idx`. \n",
    "* The data before `split_idx` will be your `train_set`\n",
    "* the data after `split_idx` becomes your `test_set`\n",
    "\n",
    "Because your data is sorted by date, this means we use the oldest 70% of the data as train set, and the rest as our test set. Go ahead and create the `split_idx`, `train_set` and `test_set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc426a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(timeseries.shape[0]*.7)\n",
    "split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac1550",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = timeseries[:split_idx]\n",
    "test_set = timeseries[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ecff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0abf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4723d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985936c4",
   "metadata": {},
   "source": [
    "### Creating the `X_train`, `y_train`, `X_test` and `y_test`\n",
    "\n",
    "Now that you hvae your `train_set` and `test_set`, you can creature the features (`X_train` and `X_test`) by taking the `lags` column that you created previously as `features`, and the target temperatures (`y_train` and `y_test`) as the values for `residuals`.\n",
    "\n",
    "Go ahead and create these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e2a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [f'{i}-months-lag' for i in range(1, 12)]\n",
    "\n",
    "X_train = train_set[lags]\n",
    "y_train = train_set.residuals\n",
    "X_test = test_set[lags]\n",
    "y_test = test_set.residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2971f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17faab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe2ea3",
   "metadata": {},
   "source": [
    "### ☑️ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc93dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('ml_data',\n",
    "                         X_train = X_train,\n",
    "                         train_set = train_set,\n",
    "                         test_set = test_set\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b16ca3",
   "metadata": {},
   "source": [
    "# First modelling attempt\n",
    "\n",
    "You are almost there!\n",
    "\n",
    "Now that you have your data, you can train a machine learning model the way you normally would. Do the following:\n",
    "\n",
    "* Create a `KNeighborsRegressor`, and `fit` it to `X_train` and `y_train`\n",
    "* calculate the `RMSE` between the `y_test` and `y_pred` (the prediction on your `X_test`): is this a reasonable temperature error?\n",
    "* Save you `RMSE` into a variable called `first_rmse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f11793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "rfor = KNeighborsRegressor().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99581006",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "first_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "first_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d3d95",
   "metadata": {},
   "source": [
    "<details><summary>🔍 Observations</summary><br>\n",
    "    \n",
    "    You should have a reasonably low error in the order of 1.2 degrees or less. Not bad, especially given that our training set is before 1940, and we predict temperatures after 1940!</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b884e8",
   "metadata": {},
   "source": [
    "## Plotting the reconstructed predicted time series\n",
    "\n",
    "Now, let's plot the `reconstructed predicted time series` against the `timeseries.AverageTemperature` value of `test_set`: remember, you need to add the `timeseries.trend` and `timeseries.season` to your `y_pred` (and you will need to slice the original data to only plot the values corresponding to the predictions).\n",
    "\n",
    "Does the prediction look good? How about if you only look at the last three years of the predictions versus the actual?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb23c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_timeseries = y_pred+timeseries.trend[split_idx:]+timeseries.season[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06954c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(25,5))\n",
    "\n",
    "ax.plot(timeseries.dt[split_idx:],timeseries.AverageTemperature[split_idx:], c='b', label='Actual')\n",
    "ax.plot(timeseries.dt[split_idx:],pred_timeseries, c='r', alpha=.5, label='predicted');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ae296",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(15,5))\n",
    "\n",
    "ax.plot(timeseries.dt[-36:],timeseries.AverageTemperature[-36:], c='b', label='Actual')\n",
    "ax.plot(timeseries.dt[-36:],pred_timeseries[-36:], c='r', alpha=.5, label='predicted');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387eec7",
   "metadata": {},
   "source": [
    "### ☑️ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f2d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('first_model',\n",
    "                         score = first_rmse\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f410a0a",
   "metadata": {},
   "source": [
    "# Second modelling: more realistic forecasting\n",
    "\n",
    "The match between the actual data and the prediction from your model should look very, very good. But are our predictions realistic? Well, what we have done really is use 11 months of actual data, and predict the temperature of the $12^{th}$ month. In other words, we forcasted 1 month in advance (`forcasting` is the term used in time series for `prediction`).\n",
    "\n",
    "But can we predict the future with as much accuracy? Our model is capable of predicting one month ahead, but if we want to predict two months, then we need to take 10 months of actual data, and then the last month is our previous prediction, for 3 months ahead, we need 9 month of actual data and 2 last months of prediction, etc... In other words, to test if we can predict the future, we need to have a rolling average of predictions. Intuitively, this should increase the uncertainty of our forecast.\n",
    "\n",
    "**LET'S TRY IT!**\n",
    "\n",
    "Now do the following:\n",
    "\n",
    "* Create a loop through all of the `y_test` values of your data that will make a prediction on the previously predicted 11 months of data\n",
    "* To do this, start your loop with a list (`X`) of the 11 previous months of actual data: these should be the 11 values of `X_train[lags]` for your first `y_test`\n",
    "* Then, at each loop, make a `y_pred` prediction based on the list of the previous loop, and append this into a prediction list. Then, remove the first value of your `X` and append to it the `y_pred` (this will be your last feature for the next prediction)\n",
    "* The goal is to always have the 11 most recent `y_pred` values as your features, and also save the current prediction as your `label`.\n",
    "\n",
    "Once you have done this, calculate a new `RMSE` value (save it as a variable named `second_rsme`), and plot your data in a similar way as before. Does the result still look good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prediction = X_train.loc[X_train.shape[0],lags].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5900f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dcb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_realistic = []\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    y = rfor.predict(pd.DataFrame(data=[X_prediction], columns=lags))[0]\n",
    "    X_prediction = X_prediction[1:]\n",
    "    X_prediction.append(y)\n",
    "    y_pred_realistic.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab5d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_pred_timeseries = y_pred_realistic+timeseries.trend[split_idx:]+timeseries.season[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e713a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_rmse = np.sqrt(mean_squared_error(timeseries.residuals[split_idx:],y_pred_realistic))\n",
    "second_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57719098",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(15, 5))\n",
    "plt.plot(timeseries.dt[-36:],timeseries.AverageTemperature[-36:], c='b', label='Actual')\n",
    "plt.plot(timeseries.dt[-36:],real_pred_timeseries[-36:], c='r', alpha=.5, label='predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8d19c",
   "metadata": {},
   "source": [
    "<details><summary>🎉 Observations</summary><br>\n",
    "    \n",
    "Ok, we se a little degradation of our performance, but we still seem to do very well indeed! Time to party?</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc01d2c",
   "metadata": {},
   "source": [
    "### ☑️ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fadd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('second_model',\n",
    "                         score = second_rmse\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a3b16",
   "metadata": {},
   "source": [
    "# Let's get real: Are our Results Really that Good?\n",
    "\n",
    "As a data scientist, you know that things need to be compared to a baseline in order to properly assess them. That is something we should have done at the very begining (but of course, I did not ask you to do it for a purpose!).\n",
    "\n",
    "So, build a `DummyRegressor` with the strategy set as the mean, calculate the `RMSE` score for this model and save it in a variable named `dummy_rmse`.\n",
    "\n",
    "What can you conclude from this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9492961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "dum_reg = DummyRegressor(strategy='mean').fit(X_train, y_train)\n",
    "dummy_rmse = np.sqrt(mean_squared_error(y_test,dum_reg.predict(X_test)))\n",
    "dummy_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f69584e",
   "metadata": {},
   "source": [
    "<details><summary>😨 Observations</summary><br>\n",
    "    \n",
    "    Well, simply predicting the <code>mean</code> of the dataset give also excellent results: the baseline is <strong style=\"color:blue\">very low for this dataset</strong>. In view of our DummyScore, we can conclude that our second model is <strong style=\"color:red\">worse than the baseline</strong>. So, we were not doing well at all! Our first score is a little bit better than the baseline, so the <code>RandomForest</code> regressor is capable of more accurate predictions if it is given the previous 11 months of observations (but not if it is applied to previous predictions).\n",
    "    \n",
    "    ⚠️ The reason for this is simple: in London, the temperature is fairly predictable once you remove the trend and the season, and the variance is relatively low. This is why a dummy mean prediction is still relatively ok. So, all that a machine learning algorithm needs to do is predict the mean and a bit of noise to be close!</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b152b63a",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Time Series forcasting is hard, but it is also a growing field in machine leanring. Knowing how to manipulate time series properly is a useful skill, and you are encouraged to explore this topic further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25faf3f",
   "metadata": {},
   "source": [
    "### ☑️ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49fe6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('dummy_model',\n",
    "                         score = dummy_rmse\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0f82d",
   "metadata": {},
   "source": [
    "# 🏁 Finished!\n",
    "\n",
    "Well done! <span style=\"color:teal\">**Push your exercise to GitHub**</span>."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
