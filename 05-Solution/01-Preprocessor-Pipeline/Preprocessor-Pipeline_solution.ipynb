{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46060349",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-cL5eOpEsbuIEkvwW2KnpXC12-PAbamr\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c19f12",
   "metadata": {},
   "source": [
    "# Preprocessor Pipeline\n",
    "### Back to Golden Plains Roadside Biodiversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccfa267",
   "metadata": {},
   "source": [
    "As our first exercise we will return to the  <a href='https://data.gov.au/data/dataset/golden-plains-roadside-biodiversity'>Golden Plains Shire (Australia) dataset</a>.<br>\n",
    "![plain](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6b/Mount_Conner%2C_August_2003.jpg/375px-Mount_Conner%2C_August_2003.jpg)\n",
    "<br>\n",
    "\n",
    "üéØ As last time, the exercise will consist of the data preparation and feature selection techniques you have learnt: we will again try to predict via linear regression the `RCACScore`. But this time, we will do it using `Pipelines`, and we don't really care that much about the final score (the goal is to demonstrate how pipelines work.\n",
    "\n",
    "The goal is to demonstrate how helpful pipelines are in making our code cleaner, and avoid repetition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f34c6d",
   "metadata": {},
   "source": [
    "üëá Load the data into this notebook as a pandas dataframe named `df`, and display its first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='1SUxBmDZF6fsu3ndrgrNI7aI2B2BEr59S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb683506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('raw_data/biodiversity.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2cefc6",
   "metadata": {},
   "source": [
    "# Remove Duplicates and Hold Out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54146670",
   "metadata": {},
   "source": [
    "Removing duplicates still **needs to be done before training a pipeline**. Why? Because if you don't do it now, once you train-test-split (hold out method) you might have data leakage. Do the following:\n",
    "1. Remove the duplicates\n",
    "2. Create the target `y` vector (`RCACScore`) and feature `X` matrix\n",
    "3. Using a 30% split for the test set, create the `X_train`, `X_test`, `y_train`, `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d180549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below ensures that all transformers return pandas dataframe - makes life easier!\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2f7e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop them in place and check\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a82f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df.RCACScore\n",
    "X = df.drop(columns=['RCACScore'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7ec043",
   "metadata": {},
   "source": [
    "# Creating our numeric pipeline\n",
    "\n",
    "Let's start by taking care of the numeric values, and do this in a pipeline. The steps we will want to implements are the following:\n",
    "1. Impute values with a `SimpleImputer`, with the default strategy\n",
    "2. Scale the values using a `StandardScaler`\n",
    "\n",
    "Create a pipeline that does just that! Call it `num_pipe`. Then, fit it to the **numeric columns of your** `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43096399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9875e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipe = make_pipeline(SimpleImputer(), StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = X_train.select_dtypes(include=np.number).columns\n",
    "num_pipe.fit(X_train[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa69f6d",
   "metadata": {},
   "source": [
    "# Applying your pipline\n",
    "\n",
    "Create two new dataframes named `X_train_prep` and `X_test_prep`,  containing respectively the values of `X_train` and `X_test` transformed by your pipeline.\n",
    "\n",
    "**Tip:** Note that the transform function of a pipeline returns a `NumPy array` by default. So, to obtain a nice looking datadframe, you will need to create it from the return numpy array and the columns of the original `X_train` and `X_test` dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep = pd.DataFrame(num_pipe.transform(X_train[num_cols]), columns=num_cols)\n",
    "X_train_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3971ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prep = pd.DataFrame(num_pipe.transform(X_test[num_cols]), columns=num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e2b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f419e",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Conclusions</summary>\n",
    "<br>\n",
    "‚ÑπÔ∏è Hopefully you can see with this simple example that <code>Pipeline</code>s make your life much easier! <br> \n",
    "    Once you have fitted your pipeline to the <code>X_train</code>, you can <code>transform</code> the <code>X_train</code>, <code>X_test</code>, or indeed any new data you need to predict!\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec8a74",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae7da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('num_pipe',\n",
    "                         X_test = X_test_prep\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a71b04",
   "metadata": {},
   "source": [
    "# Categorical Data Pipeline\n",
    "\n",
    "Let's now build a new `pipeline` and name it `cat_pipe` for **Categorical Pipeline**. We will deal with the categorical data by doing the following:\n",
    "- Apply a `SimpleImputer()` with the `most_frequent` strategy\n",
    "- Apply a `OneHotEncoder()` with the `handle_unknown=\"ignore\"` to the categorical data\n",
    "\n",
    "Then, transform the `X_train` and `X_test` categorical data into a `X_train_cat_prep` and `X_test_cat_prep` in the same way as you trensformed the numerical data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e9adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_cols = df.select_dtypes(exclude = np.number).columns\n",
    "\n",
    "cat_pipe = make_pipeline(SimpleImputer(strategy='most_frequent'), OneHotEncoder(handle_unknown='ignore', sparse_output=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43aae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe.fit(X_train[cat_cols])\n",
    "\n",
    "X_train_cat_prep = cat_pipe.transform(X_train[cat_cols])\n",
    "X_test_cat_prep = cat_pipe.transform(X_test[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b774a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d156ab",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be94d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('cat_pipe',\n",
    "                         X_test = X_test_cat_prep)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755aef94",
   "metadata": {},
   "source": [
    "# Building a single processing Pipeline\n",
    "\n",
    "Ok, the process above was already saving us a lot of time. But now, we are going to take this to the next level by creating a `ColumnTransformer` that groups both `pipelines`together! Name your `ColumnTransformer` as `preproc_pipe` (**preprocessing pipeline**). If in doubt, consult the documentation!\n",
    "\n",
    "Once your pipeline is created, you can `fit` it on the `X_train` and then `transform` the `X_train` and the `X_test` into `X_train_preproc` and `X_test_preproc`: you will see how easy it makes your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ec2c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preproc_pipe = ColumnTransformer([\n",
    "    (\"num_prep\", num_pipe, X_train.select_dtypes(include=np.number).columns),\n",
    "    (\"cat_prep\", cat_pipe, X_train.select_dtypes(exclude=np.number).columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d95a849",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_pipe.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5013d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preproc = preproc_pipe.transform(X_train)\n",
    "X_test_preproc = preproc_pipe.transform(X_test)\n",
    "X_train_preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac19fd0",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e27bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('preproc_pipe',\n",
    "                         X_test = X_test_preproc\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca87229",
   "metadata": {},
   "source": [
    "# Adding models into pipelines\n",
    "\n",
    "The true power of the `pipeline` class is that it can also handle full `sklearn` models! So, all we need to do now is create a new `pipeline`, let's call it `linear_model`, and this will contain the `preproc_pipe` and a `LinearRegression` model. Then, you can directly fit it to the `X_train` and `y_train`. Onces this is fitted, you can estimate the `y_pred` from the `X_test`, and save the `root mean squared error` of this model into a new variable, `score`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84667718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "linear_model = Pipeline([\n",
    "    ('preproc',preproc_pipe),\n",
    "('model', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linear_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac64a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7978280d",
   "metadata": {},
   "source": [
    "# üèÅ Finished!\n",
    "\n",
    "Well done! <span style=\"color:teal\">**Push your exercise to GitHub**</span>, and move on to the next one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
