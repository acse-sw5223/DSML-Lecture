{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a51641",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-hPP-XPm9_5M3orUgmompcVleQ5xvPST\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcfee22",
   "metadata": {},
   "source": [
    "# Predicting next month's temperatures\n",
    "\n",
    "In this exercise, you will use a modified version of the world's city temperature. The main goal of this exercise is to introduce you to some more advanced feature engineering techniques, notably `Feature Cross`, `Cyclic Data Encoding`, and `Feature Hashing`. Settle down: this exercise will push your coding skills a bit more!\n",
    "\n",
    "# Reading the data\n",
    "\n",
    "I have already split the data for you into a `train_set` and a `test_set` in the cell below. Note that because the data is a `Time Series` I have split it in a way that all of our `train_set` lies between 2003 and 2011m whereas the `test_set` represents the years 2012, and 2013 (we will discuss next week in greater details why it is advisable to do this - for now, just go with the flow!).\n",
    "\n",
    "Once you have executed the cell below, look at the `train_set` to see what the features we are going to work with look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc84410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='1pm9Y1uOGgRsfnhGHZUpsdSXmF3wgtXK2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f1ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell first in order to be able to see changes in your \"custom_transformers.py\" file without needing to restart your kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7951da63-ffe1-425b-8231-12db6b3faa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_set = pd.read_csv('raw_data/train_temperatures.csv')\n",
    "test_set = pd.read_csv('raw_data/test_temperatures.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352bf54-40a6-40ac-9e5e-3d9587e9b089",
   "metadata": {},
   "source": [
    "# Reducing the size of the data\n",
    "\n",
    "Some of you, especially running on WSL, will struggle with the size of this data so we will only take 25% of our train and test splits. But if you are curious, you can play with the entire dataset later (assuming you have enough memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47061afc-077d-4294-b51a-c26d89d14269",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.sample(frac=0.25, random_state=42)\n",
    "test_set = test_set.sample(frac=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02fbea4",
   "metadata": {},
   "source": [
    "## Our objective\n",
    "\n",
    "We will try to predict the `AverageTemperature` based on the different features you can see in the `DataFrame`. All of them should be relatively easy to understand intuitively. So, let's look at a visual of our data to get a sense of what we are dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af6562",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "To better understand our challenge, let's visualize it for a given month, of a given year. Do the following:\n",
    "\n",
    "1. Create a new `DataFrame` called `may_2004` that will only contain the data from your `train_set` that corresponds to the year 2004 and the month of May.\n",
    "2. Create a new feature in your `may_2004` dataframe, and call it `temp_diff`. This is equal to the `TempPreviousMonth` minus the `AverageTemperature`. In other words, it tells us what the temperature difference between the month of April 2004 and the month of May 2004 looks like. This is what we will try to predict for all months of the year based on the entire dataset.\n",
    "3. Now it is time to plot our data! Create a new figure with axes, I suggest using a `figsize=(12,8)` or something similar to be able to view the data well. Scatter `may_2004.Longitude` against `may_2004.Latitude`, and set the color of each point to be controled by `may_2004.temp_diff` (look at the parameter `c` of the matplotlib `scatter()` function. I also recommend that you use the `coolwarm_r` color map (`cmap` parameter) for easy visualization\n",
    "\n",
    "What can you see, and does it make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b39eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "may_2004 = train_set[((train_set.year == 2004) & (train_set.month == 'May'))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997e7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "may_2004['temp_diff'] = may_2004.TempPreviousMonth - may_2004.AverageTemperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd5c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,8))\n",
    "ax.scatter(may_2004.Longitude, may_2004.Latitude, c=may_2004.temp_diff, cmap='coolwarm_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3247b0a",
   "metadata": {},
   "source": [
    "<h2>üí° Conclusions</h2>\n",
    "<br>\n",
    "If you followed the instructions above, you should now see the following:<br>\n",
    "    <li>Each city plotted gives the rough contour of the continents.</li>\n",
    "    <li>In the Southern Hemisphere, the dots are mostly <span style=\"color:blue\">blue</span>, indicating negative values of temperature difference (i.e. May is cooler than April, as we are moving towards the Austral Winter.</li>\n",
    "    <li>In the Northern Hemisphere, the dots are <span style=\"color:red\">red</span>: this is Spring warming</li>\n",
    "    <li>In the Equatorial region, the dots are <span style=\"color:gray\">gray</span>: this is because temperatures are fairly equables in the tropic (i.e. don't change much)</li>\n",
    "<br>\n",
    "    So what do we learn here? That the change of temperature will depend on <strong>the season</strong> and the <strong>geographical location</strong>. Luckily, we have information on that:\n",
    "    <li><strong>Geographic location</strong>: we have the latitude and longitude of the cities, and also the city names!</li>\n",
    "    <li><strong>Season</strong>: we have information about the month when the data was measured</li>\n",
    "    \n",
    "We will look at different ways to incorporate this information in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65fc823",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d285a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('visualization',\n",
    "                         df = may_2004\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd2313",
   "metadata": {},
   "source": [
    "# Creating features and target\n",
    "\n",
    "Time for us to create our features (`X_train` and `X_test`) and our targets (`y_train` and `y_test`). Use the `AverageTemperature` as your targets for both the `test_set` and the `train_set`. And for the `X_train` and `X_test` drop the `AverageTemperature` and keep the others: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.drop(columns='AverageTemperature')\n",
    "X_test = test_set.drop(columns='AverageTemperature')\n",
    "\n",
    "y_train = train_set.AverageTemperature\n",
    "y_test = test_set.AverageTemperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f073e17d",
   "metadata": {},
   "source": [
    "# Baseline Score\n",
    "\n",
    "We need to have an idea of what score we need to beat to be able to claim that we have a model that is performing better than a random guess. In our case, a baseline model to beat would be using the temperature of the previous month as our prediction.\n",
    "\n",
    "Do the following:\n",
    "1. Calculate the `root mean squared error` between `X_test.TempPreviousMonth` and `y_test`. \n",
    "2. Save this value into a new variable named `baseline_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220029e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "baseline_score = np.sqrt(mean_squared_error(y_test, X_test.TempPreviousMonth))\n",
    "baseline_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d172c1",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dac9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('baseline_score',\n",
    "                         score = baseline_score\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7332b7",
   "metadata": {},
   "source": [
    "# Training a Linear model based on numerical coordinates\n",
    "\n",
    "In our first attempt to model the temperature a month ahead, we will use the `numerical coordinates` (i.e. `Latitude` and `Longitude`) only. We won't beat our `baseline_score` just yet: we will work towards that throughout this workbook!\n",
    "\n",
    "Do the following:\n",
    "1. Import `set_config` and use `set_config(display='diagram')` for easy visualization of your pipelines \n",
    "2. Create  a `ColumnTransformer` that contains a `StandardScaler`, and make sure to only select the columns `Latitude` and `Longitude` for this step. I suggest you call this transformer `preproc`.\n",
    "3. Combine your `preproc` and a `LinearRegression` model into a new pipeline: call it `coord_model`\n",
    "4. Train `coord_model` on `X_train` and `y_train`\n",
    "5. Calculate the `root mean squared error (RMSE)` for `coord_model` using your `X_test`. Save this value in a variable named `coord_score` (make sure to look at it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b4f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')\n",
    "\n",
    "\n",
    "preproc = ColumnTransformer([('num',StandardScaler(), ['Latitude', 'Longitude'])])\n",
    "\n",
    "coord_model = make_pipeline(preproc, LinearRegression())\n",
    "\n",
    "coord_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d730107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "coord_score = np.sqrt(mean_squared_error(y_test, coord_model.predict(X_test)))\n",
    "coord_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c3e519",
   "metadata": {},
   "source": [
    "<details><summary><strong style=\"color:darkgreen\">Are you happy with this score?</strong></summary><br>\n",
    "    This is obviously lower than our <code>baseline_score</code> but is to be expected: coordinate alone cannot be a better predictor than the previous month temperature. <br>\n",
    "    More importantly, giving <strong style=\"color:blue\">numerical values</strong> is not the best way to encode geospatial data: intruitively, <strong>BOTH</strong> the <code>Latitude</code> and the <code>Longitude</code> should be given together (not in separate columns) to pinpoint a location on the planet. In the next section, we will discuss how this information can be encoded for our machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80699e",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb44ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('num_coordinates',\n",
    "                         score = coord_score\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54213c07",
   "metadata": {},
   "source": [
    "# Introducing `Feature Crosses`\n",
    "\n",
    "This brings us to a very important concept: that of a `feature crosses`. Simply put, a feature cross is a synthetic feature that is the combination of two or more features together: independently, none of these features may be very strong predictors. But when associated, they become powerful! You can <a hread=\"https://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture#:~:text=A%20feature%20cross%20is%20a,an%20understanding%20of%20feature%20crosses.\">watch this brief video from Google</a> about `feature crosses` if you want to know more, or <a hreaf=\"https://www.linkedin.com/pulse/why-feature-crosses-still-important-machine-learning-rakesh-sharma/\">read this article</a>.\n",
    "\n",
    "In our case, the coordinates of the city give us a clear opportunity to practice feature crossing: we have both the `Latitude` and the `Longitude` of the cities in separate columns. However, a location is a combination (`feature cross`!) of both `latitude` and `longitude`. Simply creating a new column that contains a string in the form of `{Latitude}_{Longitude}` would not be enough though: we need to encode the entire space of possible parameters to create a high-dimentional feature space. Then we can represent each combination of `Latitude` and `Longitude` as a unique vector.\n",
    "\n",
    "To achieve this, you will need to write three functions: `train_encoder`, `encode_features`, and `create_feature_cross`. Let's walk through them one by one.\n",
    "\n",
    "### `train_encoder`\n",
    "\n",
    "The purpose of this function is to train a `OneHotEncoder` on all possible combinations of `Latitude` and `Longitude` rounded to the nearest full degree. The function will return a trained `OneHotEncoder`. Here are a few useful tips when writing this function:\n",
    "\n",
    "1. Remember that you can cast a `float` to an `int` to round the values: I suggest you use this approach here\n",
    "2. You might want to generate two lists: one of all possible `latitudes` (from `-90` to `+90`) and one of all possible `longitudes` (from `-180` to `+180`). **Remember:** We are trying to encode each full degree here, i.e. `int`s. The `np.arrange` function might be your best friend here!\n",
    "3. You probably want to create a list that will contain all of your possible combinations of features (let's call it `corpus`, but you can choose another name)\n",
    "4. You might want to loop through all `latitudes`, and then for each `latitude` value combine it with all possible values of `longitude`. Then append this combination as a string (for instance, `\"34_-123\"` for 34N and 123W) to your list\n",
    "5. To save you a lot of grief in your next function, I recommend you turn your `corpus` list into a `pd.DataFrame` with a column name (for instance, `coordinates`)\n",
    "6. Now, create a `OneHotEncoder` (important: do **NOT** set the parameter `sparse` to `False`. We definitely want sparsity for speed of training and memory usage), and `fit` your encoder on the `DataFrame` version of your corpus.\n",
    "7. Return your fitted `OneHotEncoder`.\n",
    "\n",
    "Check your code to see if it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd58a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# arange() and linspace()\n",
    "\n",
    "def train_encoder(min_lat=-90, max_lat=91, min_lon=-180, max_lon=180, precision=1):\n",
    "    \n",
    "    coordinates_ohe = OneHotEncoder()\n",
    "    \n",
    "    min_lon = int(min_lon/precision)\n",
    "    min_lat = int(min_lat/precision)\n",
    "    max_lon = int(max_lon/precision)\n",
    "    max_lat = int(max_lat/precision)\n",
    "    \n",
    "    lats = np.arange(min_lat, max_lat, 1)\n",
    "    lons = np.arange(min_lon, max_lon, 1)\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    for lat in lats:\n",
    "        for lon in lons:\n",
    "            corpus.append(f'{lat}_{lon}')\n",
    "    \n",
    "    coordinates_ohe.fit(pd.DataFrame(data=corpus, columns=['coordinates']))\n",
    "    \n",
    "    return coordinates_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e760d",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c79966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('encoder',\n",
    "                         encoder = train_encoder()\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656bf612",
   "metadata": {},
   "source": [
    "### `encode_features(row)`\n",
    "\n",
    "The goal of this function is to take as input a single `row` of one of your `DataFrame` (for instance, the `X_train` or `X_test`, and return a `string` of your `row.Latitude` and `row.Longitude` in the same format as you trained your `OneHotEncoder`. For instance, if you followed my example above, the function should return a string `\"23_-5\"` for the values of `Latitude=23.6` and `Longitude=-5.06`. If you encoded your data differently in your `train_encoder` function, then follow what you did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d321a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(row, precision=1):\n",
    "    \n",
    "    lat = int(row.Latitude/precision)\n",
    "    lon = int(row.Longitude/precision)\n",
    "    \n",
    "    return f'{lat}_{lon}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73fb1e7",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3641f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('encode_features',\n",
    "                         encoded_string = encode_features(pd.Series([23.6, -5.06], index=['Latitude', 'Longitude']))\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30efad0",
   "metadata": {},
   "source": [
    "### create_feature_cross(df)\n",
    "\n",
    "This function will take as input a `DataFrame` (I call it `df`), and return a sparse matrix of your coordinates encoded. Don't worry, you already did most of the hard work! Here is what you need to do in this function:\n",
    "\n",
    "1. Create an encoder by using the `train_encoder` function you wrote above. You will use this encoder to encode your data.\n",
    "2. Create a vector of strings that represents your coordinates. Because you already have written a fonction to turn coordinates into strings, it is easy. I suggest you use the `apply()` method on `df`, and use a `lambda function` to turn each row (think of setting `axis=1` in `apply()) into a string. Save the return value of this function (a vector) in a variable (I call it `vector`).\n",
    "3. Turn your vector into a `DataFrame` with one column named `coordinates`\n",
    "4. Use your `OneHotEncoder` to `transform` your `DataFrame`, and return the transformed value from your function\n",
    "\n",
    "You are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c5f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def create_feature_cross(df, precision=1):\n",
    "    encoder = train_encoder(precision=precision)\n",
    "    \n",
    "    vectors = df.apply(lambda x: encode_features(x,precision), axis=1)\n",
    "\n",
    "    vectors = encoder.transform(pd.DataFrame(data=vectors, columns=['coordinates']))\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d2472f",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7817a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "test_df = pd.DataFrame(data=[[23.6, -176.4], [-5.06, 90.0]], columns = ['Latitude', 'Longitude'])\n",
    "result = ChallengeResult('create_feature_cross',\n",
    "                         sparse_matrix = create_feature_cross(test_df)\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770feece",
   "metadata": {},
   "source": [
    "## Modelling with our `Feature Hash`\n",
    "\n",
    "Now comes the exciting time: we shall see if our `Feature Hash` is more predictive than simply using he numrical coordinaes on their own. Do the following:\n",
    "\n",
    "1. You can create  `X_train_hashed` and `X_test_hashed` by applying the `create_feature_cross()` function to `X_train` and `X_test`, respectively.\n",
    "2. Use the `X_train_hashed` to train a new model (`fc_model`) that contains only a `LinearRegression`\n",
    "3. Calculate the new `root mean squared error` and save it into a variable named `fc_score`\n",
    "\n",
    "Do we beat our old `coord_score`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b46552",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_model = LinearRegression().fit(create_feature_cross(X_train, 1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc19bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_score = np.sqrt(mean_squared_error(y_test, fc_model.predict(create_feature_cross(X_test, 1))))\n",
    "fc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b591ee",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('fc_score',\n",
    "                         score = fc_score\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea2921",
   "metadata": {},
   "source": [
    "# `CoordinatesFeatureCross` Transformer\n",
    "\n",
    "Write a class implementing a transformer that is similar to the code you produced above it aims at encoding a `Feature Cross` of your latitude and longitude coordinates (do this in the `custom_transformers.py` file using `VSCode`). It returns a sparse matrix of the encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e835cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_transformers import CoordinatesFeatureCross\n",
    "\n",
    "cfc = CoordinatesFeatureCross().fit(X_train, y_train)\n",
    "feature_cross = cfc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c83b3",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20fa978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('feature_cross_transformer',\n",
    "                         sparse_matrix = feature_cross\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd0fa67",
   "metadata": {},
   "source": [
    "# Modelling with cities name\n",
    "\n",
    "We will now take advantage of the `City` feature. Because we know that location matters for our prediction, we can surmise that adding the city name would matter too. But before we proceed with that, let's think of whether the city name in itself is a sufficiently unique feature. Look at the `unique` values in `X_train` for `City==London`. What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa8e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[X_train.City == 'London']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bd06e6",
   "metadata": {},
   "source": [
    "### ‚òùÔ∏è `City` is not unique\n",
    "\n",
    "Hopefully the above should have convinced you that the `City` feature is not unique (the same city name can appear in different countries). To work around this, create a new feature called `\"City_Country\"` that is a string that pairs the city and country with a `_` character. For instance, replace `London` by `London_United Kingdom`. Do this for both the `X_train` and the `X_test`. Do not  drop the `Country` feature and `City` yet: we will need those later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4416da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['City_Country'] = X_train.apply(lambda x: f'{x.City}_{x.Country}', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60961881",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['City_Country'] = X_test.apply(lambda x: f'{x.City}_{x.Country}', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d52c9",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fe67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('city_country',\n",
    "                         X_train = X_train,\n",
    "                         first_city=X_train.City_Country[0]\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d392320",
   "metadata": {},
   "source": [
    "## Modelling with Cities [attempt #1]\n",
    "\n",
    "Now that you have replaced the `City` feature by a more unique feature name (the `City_Country` feature), it is time to see how we can model it. Do the following:\n",
    "1. Create a numerical pipeline that contains a `StandardScaler`, and a categorical pipeline that contains a `OneHotEncoder`. We will attempt to use `OneHotEncoding` for our `City_Country` feature.\n",
    "2. Combine both pipelines into a `ColumnsTransformer`: select `['TempPreviousMonth','Latitude', 'Longitude']` as features for the numerical pipeline, and `City` for the categorical pipeline. Set `remainder='drop'` for the `ColumnTransformer`\n",
    "3. Combine your `ColumnTransformer` with a `LinearRegression()` into a new pipeline called `City_model`, and `fit` this model to `X_train` and `y_train`.\n",
    "\n",
    "You should now have a fully fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4644a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipe = make_pipeline(StandardScaler())\n",
    "cat_pipe = make_pipeline(OneHotEncoder(handle_unknown='ignore'))\n",
    "\n",
    "preproc = ColumnTransformer([('num',num_pipe, ['TempPreviousMonth','Latitude', 'Longitude']), \n",
    "                             ('cat', cat_pipe, ['City_Country'])], remainder='drop')\n",
    "\n",
    "city_model = make_pipeline(preproc, LinearRegression())\n",
    "\n",
    "city_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb829d",
   "metadata": {},
   "source": [
    "### Assessing performance of the new model\n",
    "\n",
    "Now, as you did before, try to predict the performance (`root mean squared error`) of your model based on the `X_test`. If you obtain a strange error, look at the `hint` below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14609c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_test, city_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4954660f",
   "metadata": {},
   "source": [
    "<details><summary>üõü Hint</summary>\n",
    "    If you have not set your `OneHotEncoder` with the option `handle_unknown='ignore'`, you will get a bit fat error telling you: <code><strong style=\"color:red\">ValueError</strong>: Found unknown categories ['Islamabad_Pakistan', 'Thousand Oaks_United States', 'Laohekou_China'] in column 0 during transform</code>. This means that three cities in your <code>X_test</code> where not present in your `X_train`, so are unknown.<br><br>\n",
    "    This is normal and reflects real world data: sometimes, you cannot predict at <strong>training time</strong> all of the categories you will encounter at <strong>prediction time</strong>. If you set your `OneHotEncoder` as suggested above, it will ignore these categories and consider them as `nan`. Not ideal. There is a better way: read below to know more.</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11881429",
   "metadata": {},
   "source": [
    "# Features Hashing\n",
    "\n",
    "`OneHotEncoding` is great because it gives us certainty of a 100% encoding without the risk of a clash of features; when using the `sparse_output` option, it can even handle some very large matrices fairly well, as we have seen with our `Feature Cross` before. But it has two major drawbacks:\n",
    "1. `OneHotEncoding` forces you to encode using the same number of columns as there are categories - this can become problematic for datasets with very large cardinality (think for instance of encoding all the English words using `OneHotEncoding`: impractical!)\n",
    "2. As you have discovered, if there is a feature category missing in the features you are trying to predict, `OneHotEncoding` cannot handle it and you need to set `handle_unknown=ignore`.\n",
    "\n",
    "Instead, we will use a different encoding technique called `Feature Hashing`, also known as the ***hashing trick***. <a href=\"https://en.wikipedia.org/wiki/Feature_hashing\">Wikipedia has a good page about this</a> in case you want to read more.\n",
    "\n",
    "The idea is to encode the values dataset by binning our features into a define number of categories, and then transform this into a sparse vector by applying a deterministic `hashing algorithm`. The exact details of the algorithm are not essential here (see the link above for some of the math), what matters is the principle. Note too that you can use `Feature Hashing` to reduce the dimensionality of the encoded dataset if you decide to.\n",
    "\n",
    "A few things to consider:\n",
    "\n",
    "1. One immense benefit of this approach is that categories outside of our bins will not break the code (they will just be assigned to the wrong bin)\n",
    "2. The number of bins (the `n_features` hyperparameter) is important. Too many of them, and you end up with the same cardinality as your original dataset (which is fine as long as the number of categories is not too high). Too few bins, and categories will start colliding, meaning you will introduce have more and more errors in your features.\n",
    "3. For practical applications, it is recommended to keep the number of features in the `sklearn` class as its default, or if you change it, keep the number to a power of 2 (<a href=\"https://datascience.stackexchange.com/questions/77819/how-should-i-choose-n-features-in-featurehasher-in-sklearn\">see this StackExchange post for more details</a>.\n",
    "4. It is important to realise that when we use `Feature Hashing` we sacrifice potential accuracy in exchange for practicality: either because we have a feature with too large a cardinality, or because we cannot necessarily always be sure that the categories we have at training will contain all of the categories at prediction time. If neither of these conditions are true, you moght be better off sticking with `OneHotEncoding` for maximum precision.\n",
    "\n",
    "Luckily for us, `sklearn` has a `FeatureHasher` class(<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html\">see the documentation</a>. This class does not need to be fitted (i.e. no need to call the `.fit()` method, though it exists): you can simply call the `transform()` method on the data as the hashing is deterministic and does not depend on the features.\n",
    "\n",
    "Do the following:\n",
    "1. Create a `ColumnTransformer` (let's call it `preprocessor` that contains your `numerical pipeline` from the previous step, and filters the same features as before (`['TempPreviousMonth','Latitude', 'Longitude']`)\n",
    "2. Instead of the `categorical encoder` that contained a `OneHotEncoder`, create a different categorical pipeline that contains a `FeatureHasher` (part of the `feature_extraction` module of `sklearn`). **Important:** Set the `input_type` parameter of your `FeaterHasher` to `'string'` since this is what we are using for `City_Country`\n",
    "3. If you run your pipeline with the `string` as it is, you will run into an issue. We need to provide a list of strings as input. The easiest is to quickly transform the `X_train.City_Country` column with a lambda function to be a list that contains the string (e.g. `'Kingston_Jamaica'` becomes `['Kingston_Jamaica']`). Make sure to also transform the `X_test.City_Country` for the test!\n",
    "4. combine your `preprocessor` with a `LinearRegression()` model into a new `Pipeline` named `city_model`\n",
    "5. Now `fit` the `city_model` to your `X_train` and `y_train`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:,'City_Country'] = X_train['City_Country'].apply(lambda x: [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a443d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('feature_hasher', FeatureHasher(input_type='string', ), 'City_Country'),\n",
    "    ('num',Pipeline([('scaler', StandardScaler())]), ['TempPreviousMonth','Latitude', 'Longitude'])],remainder = 'drop')\n",
    "\n",
    "city_model = make_pipeline(preprocessor, LinearRegression())\n",
    "\n",
    "city_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95efce33",
   "metadata": {},
   "source": [
    "### Assessing performance of the model with `FeatureHasher`\n",
    "\n",
    "Now, as you did before, try to predict the performance (`root mean squared error`) of your model based on the `X_test`, and this time save the results in a variable named `hash_score`. \n",
    "\n",
    "You should have no problem this time, and see a performance slightly better than your previous model (though not massively so...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beccf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.loc[:,'City_Country'] = X_test['City_Country'].apply(lambda x: [x])\n",
    "hash_score = np.sqrt(mean_squared_error(y_test, city_model.predict(X_test)))\n",
    "hash_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dde191",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5dd6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('final_score',\n",
    "                         score = hash_score\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a61de6",
   "metadata": {},
   "source": [
    "# `CityEncoder` Transformer\n",
    "\n",
    "Now you will write a complete Transfomer class for the `City` feature (do this in the `custom_transformers.py` file using `VSCode`). The transformer that will use the `City` and `Country` columns, combine them, and return a sparse matrix generated by the `FeatureHasher`. Use your new transformer to transform `X_train` and save the results under a variable named `test_cities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a0e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_transformers import CityEncoder\n",
    "\n",
    "city_encoder = CityEncoder().fit(X_train, y_train)\n",
    "\n",
    "test_cities = city_encoder.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f2a5b3",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('city_encoding',\n",
    "                         sparsity = type(test_cities),\n",
    "                         shape = test_cities.shape\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cffb28",
   "metadata": {},
   "source": [
    "# üèÅ Finished!\n",
    "\n",
    "Well done! <span style=\"color:teal\">**Push your exercise to GitHub**</span>, and move on to the next one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
