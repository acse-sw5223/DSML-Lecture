{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2838fb1",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-cL5eOpEsbuIEkvwW2KnpXC12-PAbamr\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea056f",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04756656",
   "metadata": {},
   "source": [
    "Remember the **Golden Plains Roadside Biodiversity** dataset your worked on the first day? We ended up dropping many features in our **linear regression** whilst maintaining a good $R^2$ score. We will use this dataset again here.\n",
    "\n",
    "![kangaroo](https://s.yimg.com/uu/api/res/1.2/GANJCEs2SP0QamHePbZqUw--~B/aD0zNjE7dz03Njg7YXBwaWQ9eXRhY2h5b24-/http://media.zenfs.com/en_us/News/afp.com/b9a6c5065aab22b840d60a188e7767a7ce7c471c.jpg)\n",
    "\n",
    "However, there are a few differences:\n",
    "- We will use logistic classifiers here which are easy to interpret\n",
    "- We will model the `RCACScore` as out target variable changed to a binary class: `0` indicates a score <=12, `1` a score >12.\n",
    "- The dataset is already cleaned, scaled, and one-hot-encoded for you üòå\n",
    "- The goal is to use `regularization` to detect relevant/irrelevant features based on under/overfitting criteria\n",
    "- **Our goal is to compare `L1` and `L2` penalties**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78ed2d1",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Load the data into a variable named `data`, and split it into an `X` feature matrix and a `y` target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9287bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='1cIO50NnXZg6F1Y9-aRKorKhXSS5Idnjc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21719eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c58a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"raw_data/biodiversity-prepared.csv\")\n",
    "\n",
    "# the dataset is already one-hot-encoded\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build X and y\n",
    "\n",
    "y = data[\"RCACScore\"]\n",
    "X = data.drop(columns=\"RCACScore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a43477b",
   "metadata": {},
   "source": [
    "## Logistic Regression without regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ffb72f",
   "metadata": {},
   "source": [
    "‚ùì Rank the feature by decreasing order of importance according to a simple **non-regularized** Logistic Regression\n",
    "\n",
    "- Careful, `LogisticRegression` is penalized by default\n",
    "- Increase `max_iter` to a larger number until the model converges\n",
    "- remember that you can access the coefficients of the regression by calling `.coef_` on your trained model. \n",
    "- *Hint*: it might help to put the coefficient of the model in a dataframe with column names from `X` to be able to interpret them. Also check the `transpose()` and `sort_values()` pandas functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10aab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(penalty=None, max_iter = 1500)\n",
    "\n",
    "model.fit(X,y)\n",
    "\n",
    "df = pd.DataFrame(np.abs(model.coef_), columns=X.columns)\n",
    "df=df.transpose()[0].sort_values(ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e593796",
   "metadata": {},
   "source": [
    "‚ùìHow do you interpret, in plain english language, the value for the coefficient `RCACRareSp` ?\n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "> \"All other things being equal (i.e. if the other variables are the same),\n",
    "the abundance of rare species (`RCACRareSp`) increases the log-odds of the site being classified as important by 33.38 (your coef value)\"\n",
    "    \n",
    "> \"Controling for all other explaining factors available in this dataset,\n",
    "a high `RCACRareSp` increases the odds-ratio of a high score by exp(33.38) = 3.14E15\"\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7944aaed",
   "metadata": {},
   "source": [
    "‚ùì What are the 5 features that most impact the chances of classifying a site as a high scoring site? Save your answer as an array under a variable named `base_most_important`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_most_important = df.head(5).index.values\n",
    "base_most_important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d1890",
   "metadata": {},
   "source": [
    "‚ùì Now cross validate a model with the same parameters as the model above, and save the mean score under a variable named `base_model_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cross_val = cross_validate(estimator=LogisticRegression(penalty=None, max_iter = 1500),X=X, y=y, cv=5)\n",
    "\n",
    "base_model_score = cross_val['test_score'].mean()\n",
    "base_model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b6858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('unregularized', \n",
    "                         top_features = base_most_important,\n",
    "                            score=base_model_score)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ef261",
   "metadata": {},
   "source": [
    "## Logistic Regression with a L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4af955",
   "metadata": {},
   "source": [
    "Let's use a **Logistic model** whose log-loss has been penalized with a **L2** term to figure out the **most important features** without overfitting.  \n",
    "This is the \"classification\" equivalent to the \"Ridge\" regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0f3b5",
   "metadata": {},
   "source": [
    "‚ùì Instantiate a **strongly regularized** `LogisticRegression` and rank its feature importance\n",
    "- By \"strongly regularized\" we mean \"more than sklearn's default applied regularization factor\". \n",
    "- Default sklearn's values are very useful orders of magnitudes to keep in mind for \"scaled features\"\n",
    "- We suggest trying a regularization factor of 10% of the default value in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l2 = LogisticRegression(penalty='l2', C=.1, max_iter = 1500)\n",
    "\n",
    "model_l2.fit(X,y)\n",
    "\n",
    "df=pd.DataFrame(np.abs(model_l2.coef_), columns=X.columns).transpose()[0].sort_values(ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c5cfef",
   "metadata": {},
   "source": [
    "‚ùì What are the top 5 features driving chances of survival according to your model ? Save them as an array under the variable name `l2_most_important`. Are these the same features as for `base_most_important`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your top 5 features below\n",
    "l2_most_important = df.head(5).index.values\n",
    "l2_most_important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a3e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RCACRareSp is still number 1 feature of importance but the others change!\n",
    "base_most_important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26417f1",
   "metadata": {},
   "source": [
    "‚ùì Now cross validate a model with the same parameters as the model above, and save the mean score under a variable named `l2_model_score`. What can you say about the new score compare to the `base_model_score`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val = cross_validate(estimator=LogisticRegression(penalty='l2', C=.1, max_iter = 1500),X=X, y=y, cv=5)\n",
    "\n",
    "l2_model_score = cross_val['test_score'].mean()\n",
    "l2_model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d36c4",
   "metadata": {},
   "source": [
    "#### üß™ Test your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26485408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('ridge', \n",
    "                         top_features = l2_most_important,\n",
    "                        score=l2_model_score)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da2071",
   "metadata": {},
   "source": [
    "## Logistic Regression with a L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa5de30",
   "metadata": {},
   "source": [
    "This time, we'll use a logistic model whose log-loss has been penalized with a **L1** term to **filter-out the less important features**.  \n",
    "This is the \"classification\" equivalent to the **Lasso** regressor\n",
    "\n",
    "‚ùì Instantiate a **strongly regularized** `LogisticRegression` and rank its feature importance. We suggest that you use the same regularization value as for **L2** to be able to compare your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2800df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1 = LogisticRegression(penalty='l1', C=.1, max_iter = 1500, solver='liblinear')\n",
    "\n",
    "model_l1.fit(X,y)\n",
    "\n",
    "df = pd.DataFrame(np.abs(model_l1.coef_), columns=X.columns).transpose()[0].sort_values(ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef975ba5",
   "metadata": {},
   "source": [
    "‚ùì What are the features that have absolutely no impact on chances of survival, according to your L1 model?\n",
    "- Save them as in a array variable named `zero_impact_features`\n",
    "- Do you notice how some of them were \"highly important\" according to the non-regularized model ? \n",
    "- From now on, we will always regularize our linear models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_impact_features = df[df==0].index.values\n",
    "zero_impact_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7cc78d",
   "metadata": {},
   "source": [
    "‚ùì Now cross validate a model with the same parameters as the model above, and save the mean score under a variable named `l1_model_score`. What can you say about the new score compare to the `base_model_score` and `l2_model_score`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11618557",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val = cross_validate(estimator=LogisticRegression(penalty='l1', C=.1, max_iter = 1500, solver='liblinear'),\n",
    "                           X=X, y=y, cv=5)\n",
    "\n",
    "l1_model_score = cross_val['test_score'].mean()\n",
    "l1_model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574d19b",
   "metadata": {},
   "source": [
    "üí° Have you noticed how the `l1_model_score` is slightly higher than the `l2_model_score` but using much less features, and that the `l2_model_score` itself higher than the `base_model_score` score? This is why regularization is so important: by filtering out the unecessary variables (i.e. setting their coefficient to zero) **L1** regularization has improved our classification score! Of course, this also comes down to the choice of the hyperparameter C, and it is possible to over-regularize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c180e",
   "metadata": {},
   "source": [
    "#### üß™ Test your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('lasso', \n",
    "                         zero_impact_features = zero_impact_features,\n",
    "                        score=l1_model_score)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c61b86a",
   "metadata": {},
   "source": [
    "# GridSearch the best hyperparameters\n",
    "\n",
    "So ***how*** do we determine the best hyperparameters for our algorithm? We can use `GridSearchCV` for that! \n",
    "\n",
    "For instance, which one of the L1 or L2 regularization is best for our performance? Or maybe we are looking at a mix of L1 and L2, known as `elastic net`? We can find out! Do a `GridSearchCV` for a logistic regression model initiated with the following arguments: `max_iter=5000`, `random_state=42`, `penalty='elasticnet'`, `solver='saga'`. Saga is the only solver that will work with elastic net. Then, find the best `LogisticRegression` model by testing the following hyperparameters in gridsearch:\n",
    "\n",
    "1. C = [1, 0.1, 0.01, 0.001]\n",
    "2. class_weight = [None, 'balanced']\n",
    "3. multi_class = ['multinomial','ovr']\n",
    "4. l1_ratio:[0, 1, 0.9, 0.7, 0.5, 0.2]\n",
    "\n",
    "Try to understand these parameters by reading the documentation, and then fit your GridSearchCV on `X` and `y`. Save the best estimator in a variable called `best_estimator`, the best parameters (as a dictionary) in a variable called `best_params`, and the accuracy score in a variable called `best_score` (hint: all of these values can be obtained from your fitted grid search model). Then test your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f558de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = {\n",
    "    'C':[1,.1,.01, .001],\n",
    "    'class_weight':[None,'balanced'],\n",
    "    'l1_ratio':[0,1,.2,.5,.7,.9],\n",
    "    'multi_class':['multinomial','ovr']\n",
    "}\n",
    "\n",
    "search = GridSearchCV(estimator=LogisticRegression(max_iter=5000, random_state=42, penalty='elasticnet', solver='saga'), \n",
    "                      param_grid=grid, scoring='accuracy', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35391c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = search.best_estimator_\n",
    "best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc75aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = search.best_score_\n",
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64488e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29234a7",
   "metadata": {},
   "source": [
    "#### üß™ Test your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('gridsearch', \n",
    "                        score = best_score,\n",
    "                        params=best_params)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c73898",
   "metadata": {},
   "source": [
    "# üèÅ Finished!\n",
    "\n",
    "Well done! <span style=\"color:teal\">**Push your exercise to GitHub**</span>, and move on to the next one."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
