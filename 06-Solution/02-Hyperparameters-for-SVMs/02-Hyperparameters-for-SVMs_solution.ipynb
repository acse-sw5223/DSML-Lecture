{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a2c541",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-cL5eOpEsbuIEkvwW2KnpXC12-PAbamr\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb2719",
   "metadata": {},
   "source": [
    "# Hyperparameters for SVMs\n",
    "\n",
    "The purpose of this exercise is to become familiar with support verctor classifier, and exploring the different classes of algorithms we can use. Support vector machine algorithms can be very computationally intensive, especially when using non-linear kernels. Our dataset today is modest, but it does have 100'000 instances. This dataset size requires us to be mindful about what algorithm we deploy.\n",
    "\n",
    "We will also use this as a good excuse to learn and practice hyperparameter tuning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb910e",
   "metadata": {},
   "source": [
    "👇 Consider the following dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b730b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='1cbL-nzeziC8SgATbxroSITfh5ukbucWW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e9906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"raw_data/data.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d6cac",
   "metadata": {},
   "source": [
    "Each observations represents the characterstic of a wine, and the last column is the quality rating [from 0 being the poorest to 10 being the best].\n",
    "\n",
    "The dataset is large (about 100'000 instances) but has missing values. The goal is to build a recommender system that will classify an unknown wine quality as either 'high' or 'low'. And we will do this in a pipeline.\n",
    "\n",
    "# Data preparation\n",
    "As you can see, right now you have 10 classes representing the wine quality. We need to turn this into a binary problem. Do the following:\n",
    "* We will consider wines with a quality rating <6 as being of 'poor' quality, and above or equal to 6 as being of 'good' quality\n",
    "* Use the <a href='https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html'>`.apply()` function in Pandas</a> and a <a href='https://towardsdatascience.com/apply-and-lambda-usage-in-pandas-b13a1ea037f7'>`lambda function`</a> \n",
    "* Replace the values in the `quality rating` column by 0 if <6 or 1 if >=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['quality rating'] = data['quality rating'].apply(lambda x: 0 if x<6 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d13be",
   "metadata": {},
   "source": [
    "## Strategy\n",
    "\n",
    "As stated in the introduction, we will need to be efficient and clever about what we do given the size of the dataset. For instance, we might not want to do cross validation on our entire dataset since it is very large, and SVMs can be slow. Also, choosing what *falvor* of SVM Classifier we use is important. What we will do is the following:\n",
    "\n",
    "1. train/test split the whole dataset with 20% test split (you can reduce the test split slize as your dataset becomes larger)\n",
    "2. Sample a small portion of the train set (**10% in our case**) as the data as our `validation_set`. Note that this will only be used to speed up some of our tests: we will use cross validation for hyperparameter searching. This means that **you should not replace the `X_train` and `y_train`** that you made in step 1. Instead, you can use train-test-split on the `train_set` but assign the 90% of the train set to the variable `_` (underscore). This is a convention in Python for a variable we won't be using.\n",
    "3. We will test the speed of different versions of `SVMs` using the `validation_set` (10% of your `train_set`)\n",
    "4. We will then do hyperparameter searches on the fastest algorithm using the original `train_set`\n",
    "5. Finally, we can train the final version of our algorithm on our `train_set` and test it on our `test_set`.\n",
    "\n",
    "<span style=\"color:blue\">**Ready? Let's do it!**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758b623e",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "To save you some time, the code for the data preparation is already given to you. I want you to focus more on the SVMs and on hyperparameter tuning in this exercuse. We will do the following:\n",
    "- Create a target (`y`) containing the `data['quality rating']`\n",
    "- Create a feature (`X`) vector containing all columns except the `data['quality rating']` \n",
    "- Train/test split the data (80%/20%)\n",
    "- Further train-test-split the `train_split` into a `validation_split` (10%) and `_` (dummy variable)\n",
    "- You should end up with these: `X_train`, `y_train`, `X_test`, `y_test`, `X_val`, `y_val` (and a `_` variable)\n",
    "\n",
    "\n",
    "Then, create a simple pipeline to do the following:\n",
    "- Impute missing values using a `SimpleImputer`\n",
    "- Scale the features using a `StandardScaler` scaler\n",
    "\n",
    "Onces you have your pipeline fitted, use it to transform the `X_train`, `X_test`, and `X_val` into variables `X_train_prep`, `X_test_prep`, and `X_val_prep` (we will need the original variables later in the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c628c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output = \"pandas\")\n",
    "\n",
    "y = data['quality rating']\n",
    "X = data.drop(columns='quality rating')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2)\n",
    "_, X_val, _, y_val = train_test_split(X_train,y_train, test_size=.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e43cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca14724",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_pipe = make_pipeline(SimpleImputer(),StandardScaler())\n",
    "prep_pipe.fit(X_train)\n",
    "\n",
    "X_train_prep = prep_pipe.transform(X_train)\n",
    "X_test_prep = prep_pipe.transform(X_test)\n",
    "X_val_prep = prep_pipe.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27741a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a066f7",
   "metadata": {},
   "source": [
    "# Testing different SVMs training speed\n",
    "\n",
    "***Aims:*** Our aim here is to see if there are any differences in how fast the different versions of **SVMs** work. Note that we will limit ourselves to *<span style=\"color:teal\">linear SVMs</span>, i.e. no other kernels.\n",
    "\n",
    "You will test the following classes of SVM: `SVC(kernel=linear)`, `LinearSVC(dual=True)`, and `SGDClassifier(loss=\"hinge\")`. For each of the algorithm, do the following:\n",
    "\n",
    "1. Instanciate a classifier without thinkering the hyperparameters in a separate notebook cell\n",
    "2. Create a new cell, and in that cell, add the following line of code first: `%%timeit`. This will ensure that the code will be run multiple times, and the average time of the code cell will be return. This will allow you to know how fast/slow it is to train each algorithm.\n",
    "3. Now `.fit()` your algorithm using `X_val_prep` and `y_val`\n",
    "\n",
    "record the timing of each of your experiment, and save the name of the algorithm as a string (`svc`,`linearsvc`,`sgd`) in a variable called `fast_algorithm`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3735d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c61ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "svc.fit(X_val_prep, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a55905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lin_svc = LinearSVC(dual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5f162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "lin_svc.fit(X_val_prep, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac32f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_svc = SGDClassifier(loss='hinge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9af84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sgd_svc.fit(X_val_prep, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f6b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_algorithm = 'sgd'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6a05a6",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Explanation</summary>\n",
    "\n",
    "- The `SVC()` algorithm supports kernels (including the non-linear kernels), but it comes at a cost: under the hood, it uses a library known as `libsvm`, which uses a **one-vs-one** strategy when fitting multi-class classifiers (slower)\n",
    "- The `LinearSVC` is based on `liblinear` and uses a **one-vs-all** approach and penalises the intercept. **It does not support kernels**. But it is much faster then `SVC()` and in fact the `dual=False` version is even a bit faster than `sgd` for our dataset (but I asked you to test `dual=True`).\n",
    "- The `SGDClassifier` with `hinge` loss also does **not support kernels** but uses stockastic gradient descent to solve for the loss, which is very fast (**SGD** is what we will use for deep-learning)\n",
    "    \n",
    "As you can see, the choice of which algorithm to use should be part of your strategy. If you need to use **kernels**, you will need to stick with `SVC()`. For very large datasets, `SGDClassifier(loss=\"hinge\")` is your best bet.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45d4256",
   "metadata": {},
   "source": [
    "### ☑️ Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9526bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('svms_speed',\n",
    "                         fast_algorithm = fast_algorithm\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78cd19",
   "metadata": {},
   "source": [
    "# Finding the best parameters and the best model\n",
    "\n",
    "Now create your `RandomizedSearchCV` object (let's call it `rand_search`). Remember to fill the 'params' dictionary, and use this guideline for that:\n",
    "* A `penalty` term between either [`l1` , `l2`]\n",
    "* A range of value of `alpha` between 0.000001 and 10 is reasonable; Explore the `scipy.stats.uniform` class and it's documentation in order to generate a uniform distribution for your values of `alpha`.\n",
    "* `max_iter` values of [1000, 5000, 10000].\n",
    "* Use `accuracy` as your scoring metric, and a value of `cv=5`\n",
    "* In a separate cell, run 60 models with your `RandomizedSearchCV` (`fit` to `X_train_prep` and `y_train`), and don't forget to use all of the CPUs on your machine to speed up training (`n_jobs=-1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5290bcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "svc = SGDClassifier(loss='hinge' )\n",
    "\n",
    "params = {\n",
    "    'penalty':['l1', 'l2'],\n",
    "    'alpha':stats.uniform(0.000001,10),\n",
    "    'max_iter':[1000,5000,10000],\n",
    "    'eta0':[0.0001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "rand_search = RandomizedSearchCV(svc,cv=5,n_iter=60,n_jobs=-1, \n",
    "                                param_distributions=params, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53069434",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search.fit(X_train_prep,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34fd7b2",
   "metadata": {},
   "source": [
    "#### Take note of how long this took\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa3082",
   "metadata": {},
   "source": [
    "# Finding hyperparameters for pipeline\n",
    "\n",
    "We can also use `RandomizedSearchCV` object (let's call it `rand_search_prep`) in pipelines! This is much more convenient as you no longer need to pre-transform your features, and you can also check the impact of your data preparation decisions. <br>\n",
    "Do the following:\n",
    "1. Combine your `data preparation` pipeline with an `SGDClassifier(loss='hinge' )` classifier into one pipline.\n",
    "2. Then, gridsearch the following parameters:<br>\n",
    "**For the classifier (as before):**\n",
    "* A `penalty` term between either [`l1` , `l2`]\n",
    "* A range of value of `alpha` between 0.000001 and 10 is reasonable; Explore the `scipy.stats.uniform` class and it's documentation in order to generate a uniform distribution for your values of `alpha`.\n",
    "* `max_iter` values of [1000, 5000, 10000].\n",
    "* Use `accuracy` as your scoring metric, and a value of `cv=5`\n",
    "* In a separate cell, run 60 models with your `RandomizedSearchCV` (`fit` to your original `X_train` and `y_train`), and don't forget to use all of the CPUs on your machine to speed up training (`n_jobs=-1`)<br>\n",
    "**For the data pipeline:**\n",
    "* Check whether a `StandardScaler()` or a `RobustScaler()` is best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089877ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipe = make_pipeline(prep_pipe, SGDClassifier(loss='hinge' ))\n",
    "final_pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "params = {\n",
    "    'sgdclassifier__penalty':['l1', 'l2'],\n",
    "    'sgdclassifier__alpha':stats.uniform(0.000001,10),\n",
    "    'sgdclassifier__max_iter':[1000,5000,10000],\n",
    "    'sgdclassifier__eta0':[0.0001, 0.01, 0.1],\n",
    "    'pipeline__standardscaler':[StandardScaler(), RobustScaler()]\n",
    "}\n",
    "\n",
    "rand_search_prep = RandomizedSearchCV(final_pipe,cv=5,n_iter=60,n_jobs=-1, \n",
    "                                param_distributions=params, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17434022",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search_prep.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26633a3d",
   "metadata": {},
   "source": [
    "## Best hyperparameters and best estimator\n",
    "\n",
    "Use the `best_params_` attribute of you `rand_search_prep` to discover what the best hyperparameters were. Then, create a variable (`rs_model`) and assign to it the value of your `best_estmator_`. The `best_estmator_` returns a trained model, so this is now your best `SVM` trained model.\n",
    "\n",
    "Finally, using your `rs_model` calculate the `accuracy_score` of the `test_set` and save this value into a variable named `rs_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc31a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search_prep.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_model = rand_search_prep.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd13b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rs_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rs_score = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c7c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d043497",
   "metadata": {},
   "source": [
    "### ☑️ Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c7b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('score',\n",
    "                         score = rs_score\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb8dc6d",
   "metadata": {},
   "source": [
    "# 🏁 Finished!\n",
    "\n",
    "Well done! <span style=\"color:teal\">**Push your exercise to GitHub**</span>, and move on to the next one."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
