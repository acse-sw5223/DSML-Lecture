{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc2f0ea",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-cL5eOpEsbuIEkvwW2KnpXC12-PAbamr\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d629554",
   "metadata": {},
   "source": [
    "#  üõ∞Ô∏è PCA to Detect Dunes in Satellite Images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6910060b",
   "metadata": {},
   "source": [
    "<img src=\"img/Sand-Dunes.jpg\" style=\"width:800px\">\n",
    "\n",
    "This challenge is based on a real-world data science problem: to detect dunes in satellite images. I worked on this problem to be able to detect dunes on Mars. My approach was to use computer vision (a subfield of deep-learning) with a model trained on Earth Dunes, and then deploy this algorithm to Martian satellite imagery. \n",
    "\n",
    "üì∫ If you are interested in this research project, you can <a href=\"https://youtu.be/3u-cOAyKocQ\">watch my 15 minutes YouTube video</a> of a talk I gave at the AGU Meeting in 2021.\n",
    "\n",
    "Here, I am giving you a much reduced version of my dataset, both in terms of number of image, and size of the images. The satellite images come from the **Sentinel 2** dataset, and were reduced to greyscale.  We will use conventional statistical machine learning to determine whether the image contains a dune (class '1') or not (class '0').\n",
    "\n",
    "We will also use PCA to reduce the dimensionality of our dataset, and convince ourselves of the benefit of doing that.\n",
    "\n",
    "This exercise is a bit different then others, in the sense that you are not dealing purely with tabular data. So keep this in mind:\n",
    "- each image is an observation (sample)\n",
    "- each pixel's luminosity level is a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea182cc",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "To make life simpler for you, I created a data loader class that will unzip the zipped file in the current directory, load each images within the unzipped file, and return a properly splitted `train_images`, `test_images`, `y_train`, `y_test`. Simply run the code cell below to load the data in this notebook.\n",
    "\n",
    "The code to load the data is not overly complicated, and I would encourage you to look at it (in `dunes_dataset.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b50536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='11_qI2fvug7ddAjw2Ik02le6OJuvf27O9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb45e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dunes_dataset import DunesDataLoader\n",
    "\n",
    "train_images, test_images, y_train, y_test = DunesDataLoader().get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee27a37",
   "metadata": {},
   "source": [
    "# Exploring the dataset\n",
    "\n",
    "First, look at the shape of the `train_images`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b07da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bf9678",
   "metadata": {},
   "source": [
    "You can see that you have **2576 images**, each of **64 x 64 pixels**. Let's see some example images. Write code to plot 25 images in an image matrix of 5 x 5 images, and write the image label as the title for each tile (**tip**: `plt.subplots` is your friend here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(5,5,figsize=(8,8))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.set_title(\"Dune\" if y_train[i]==1 else \"Not Dune\", size=12)\n",
    "    ax.imshow(train_images[i], cmap='Greys')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a09d58",
   "metadata": {},
   "source": [
    "# Preparing our `X_train` and `X_test`\n",
    "\n",
    "You will see in `deep-learning` that it is possible to use `convolutional neural networks` to input features from images to a neural network. But in statistical machine learning, this is not possible: each feature needs to be a number. So what are our features here? Well, it is the intensity of the pixel values! \n",
    "\n",
    "This means that each image has 64 x 64 = **4096 features**. There are two issues that need to be sorted before we can use the data in `train_images` and `test_images`:\n",
    "1. The pixels are arrange in a matrix (dimension of 64x64). We need all the pixel lined up as individual feature. That can surted by using `np.reshape` and reshaping both the `train_images` and `test_images` to an appropriate shape.\n",
    "2. We need to normalize the values: right now, the pixel intensity of the images ranges from 0 to 255. Once the arrays are reshaped, it is easy to simply devide their values by 255: this will result in a `MinMax` scaling (values between 0 and 1).\n",
    "\n",
    "Create your `X_train` and `X_test` by following the instructions above on `train_images` and `test_images` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_images.reshape(train_images.shape[0], 64*64)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_images.reshape(test_images.shape[0], 64*64)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d911ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb049a58",
   "metadata": {},
   "source": [
    "### üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80028ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('reshaped', shape=X_train.shape)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b091390",
   "metadata": {},
   "source": [
    "## Compress images with linear PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad59a55",
   "metadata": {},
   "source": [
    "This dunes dataset comprises images of 64 √ó 64 pixel images (4096 dimensions). Given that you have only 2576 training images, you will have very high sparsity in your data representation (few pixels of the same value in different images): the **curse of dimensionality** predicts that your machine learning algorithm will not do very well. But don't take my word for it: you will test this for yourself.\n",
    "\n",
    "What we will do is use PCA to reduce the dimensions of the data.\n",
    "\n",
    "**Apply PCA to the dataset (both `X_train` and `X_test`)**, to reduce dimensions to 100, by setting `n_components=100`. Put your transformation into variables named `X_train_projected` and `X_test_projected` (we will use the original `X_train` and `X_test` below so don't replace them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781358da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "X_train_projected = pca.fit_transform(X_train)\n",
    "X_test_projected = pca.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eff0280",
   "metadata": {},
   "source": [
    "## Thinking in terms of dimensionality reduction\n",
    "\n",
    "It is important to think about what we have done above. We have reduced the images represented by the original 4096 pixels (or 4096 dimensions) into just 100 dimensions (the 100 first **principal components** of the images). Remember from your `Numerical Mathematics` and my own module that what we call components are directions of most variance of the dataset. \n",
    "\n",
    "Reducing the 4096 pixels  to describe each images into just 150 values is a gain in dimensionality by factor about 40!\n",
    "\n",
    "**How does it work?**\n",
    "\n",
    "- The pca has found to be the most representative directions of what distinguishes each images between each other with just 100 values for every image. \n",
    "\n",
    "- They are the directions of most variance. \n",
    "\n",
    "- You can access them in `pca.components_`\n",
    "\n",
    "üëâ  Look at the first component of this array of components, and its shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb42ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07276f32",
   "metadata": {},
   "source": [
    "As you can see, it's a vector of 4096 values. We have now 100 components of 4096 values each.\n",
    "\n",
    "One satellite image is now described as a linear combination (sum) of those components.\n",
    "\n",
    "Let's reconstruct one satellite image from its reduced representation to see how it works.\n",
    "\n",
    "üëâ Use `inverse_transform` on your `data_projected` to reconstruct a `data_reconstructed` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bca5c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reconstructed = pca.inverse_transform(X_train_projected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8bc56d",
   "metadata": {},
   "source": [
    "üëâ Plot the 1st picture of the reconstructed dataset, and compare it with the original one. \n",
    "\n",
    "<details>\n",
    "    <summary>üí°Hint</summary>\n",
    "\n",
    "You'll have to reshape the flattened data into an \"image\" with the appropriate pixel dimensions (64x64)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0001a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(8, 4))\n",
    "\n",
    "axes[0].imshow(X_train[0].reshape(64,64),cmap=plt.cm.gray)\n",
    "axes[1].imshow(data_reconstructed[0].reshape(64,64),cmap=plt.cm.gray);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9157e97",
   "metadata": {},
   "source": [
    "### üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661032b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('projection', shape=X_train_projected.shape)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d57d6c",
   "metadata": {},
   "source": [
    "## Investigate your Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91630aac",
   "metadata": {},
   "source": [
    "üëâ Image-plot the \"mean\" satellite image in the dataset\n",
    "\n",
    "<details>\n",
    "    <summary>üí°Hint</summary>\n",
    "\n",
    "\n",
    "You can use `pca.mean_`\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ecdf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pca.mean_.reshape(64,64),cmap=plt.cm.gray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5417949",
   "metadata": {},
   "source": [
    "üëâ Access your first PC. What's its shape? Print it as pd.Series or NDarray. What does each values represents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9196cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(pca.components_[0])\n",
    "# Each value represent the value of PC1 for a given pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3de2ad",
   "metadata": {},
   "source": [
    "Each PC is a flatten \"image\" of 4096 pixels\n",
    "\n",
    "- Your first PC are the most important \"directions\" on of your 4096-dimension dataset.\n",
    "\n",
    "- They are the most important \"linear combination of your 4096 pixels\".\n",
    "\n",
    "- The ones which preserves the most \"variance\" when your dataset of pictures is projected onto it.\n",
    "\n",
    "- The first few PCs are the regions of the 2D pixel grid that bear the most differences between your 2576 images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d7dbf",
   "metadata": {},
   "source": [
    "üëâ Image-Plot the **5 first** principal components, as well as the **last** one.\n",
    "Do you see more intuitively what PC are? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8d9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,2, figsize=(12, 12))\n",
    "\n",
    "for k, ax in enumerate(axes.flatten()):\n",
    "    if k == 6:\n",
    "        k=150\n",
    "    ax.imshow(pca.components_[k].reshape(64,64),cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc50fad",
   "metadata": {},
   "source": [
    "Every image can be represented by the \"mean satellite image\" plus a linear combination of the 100 \"PC satellite image\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fbb7da",
   "metadata": {},
   "source": [
    "## How to Choose the Number of Components?\n",
    "\n",
    "In practice, it is very important to find how many components are needed to describe the data without losing too much information. This can be determined visually by plotting the cumulative sum of `explained_variance_ratio_` as a function of the number of components.\n",
    " \n",
    "üëâ Plot it below for the first 100 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113159ac",
   "metadata": {},
   "source": [
    "This curve quantifies how much of the total variance is contained within the first components. For example:\n",
    "- The first few components contain more than 80% of the variance,\n",
    "- while we need about only a few components to describe 95% of the variance!\n",
    "\n",
    "This means we have a great opportunity here to reduce the data further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a22344",
   "metadata": {},
   "source": [
    "**‚ùì What is the minimal number of components you need to keep to get **at least** 95% of the variance?  Assign the value to a variable called `minimal_pc_count`**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf7ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_pc_count = 0\n",
    "cum_sum = 0\n",
    "\n",
    "for i,pc in enumerate(pca.explained_variance_ratio_):\n",
    "    cum_sum+=pc\n",
    "    if minimal_pc_count==0: \n",
    "        if cum_sum >= .95:\n",
    "            minimal_pc_count=i+1\n",
    "            break\n",
    "\n",
    "minimal_pc_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d388a260",
   "metadata": {},
   "source": [
    "### üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('components', min_pc = minimal_pc_count)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba38d61",
   "metadata": {},
   "source": [
    "## PCA as feature engineering\n",
    "\n",
    "From the test you did above, you now know how many components are needed to capture 95% of the variance of your image dataset. This means we you use this value for `n_components` in a new `PCA` and then use the `principal components` as features for a classification task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c8df3",
   "metadata": {},
   "source": [
    "### Transform your training set to reduce the number of dimensions / features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96593691",
   "metadata": {},
   "source": [
    "üëâ Fit a PCA __over the training data only__ and transform your `X_train` and `X_test` into the reduced dimension (the value you found above). Call your transformed components `X_train_red` and `X_test_red` (for 'reduced')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e74bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=27)\n",
    "X_train_red = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e0e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_red = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a55ff",
   "metadata": {},
   "source": [
    "# Testing the impact of PCA on dune classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef98de",
   "metadata": {},
   "source": [
    "Here we will do two things:\n",
    "\n",
    "* 1Ô∏è‚É£ We will check how `PCA` impacts our training time\n",
    "* 2Ô∏è‚É£ We will check how `PCA` impacts our predictions\n",
    "\n",
    "We will limit ourselves to a simple linear classification algorithm (`LogisticRegression`).\n",
    "\n",
    "First, using the `%%timeit` magic function, train a `LogisticRegression` model with the following parameters:\n",
    "* `max_iter = 5000`\n",
    "* Trained on `X_train` and `y_train` (the original features)\n",
    "\n",
    "Take note of the training time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9636a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "LogisticRegression(max_iter=5000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec4da0",
   "metadata": {},
   "source": [
    "Now, also using the `%%timeit` magic function, train a `LogisticRegression` model (call it `pca_lr`) but this time with the following parameters:\n",
    "* `max_iter = 5000`\n",
    "* Trained on `X_train_red` and `y_train` (the PCA features)\n",
    "\n",
    "Take note of the training time again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c5698",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "LogisticRegression(max_iter=5000).fit(X_train_red, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8559cfb",
   "metadata": {},
   "source": [
    "### Difference in training time\n",
    "\n",
    "You should see an interesting difference in training time (`PCA` should be much faster). Below, save the **ratio** of the `full_lr` training time over the `pca_lr` training time in a variable called `time_ratio`: make sure to use the **same units** when you do this ratio. By how many order of magnitude is PCA training faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1edcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_ratio = 3.95*1000/16.7\n",
    "time_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8892ca5c",
   "metadata": {},
   "source": [
    "## Difference in performance\n",
    "\n",
    "When you execute code into a `%%timeit` cell, you effectivelly execute a loop (similar to a cross-validation). This means that the two models we trained before are no longer in memory (or more precisely, they are not available in the *scope* of this notebook).\n",
    "\n",
    "Retrain a `LogisticRegression` model with the full `X_train` and call it `full_model`, and retain a model using the `X_train_red` and call it `pca_model`. This time, don't worry about the timing.\n",
    "\n",
    "Then, produce an `accuracy_score` for your `X_test` using both the `full_model` model and the `pca_model` model. Save their respective `accuracy_score` into variables named `full_score` and `pca_score`. What can you conclude? Is the `PCA` score degraded? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = LogisticRegression(max_iter=5000).fit(X_train, y_train)\n",
    "pca_model = LogisticRegression(max_iter=5000).fit(X_train_red, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95de5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "full_score = accuracy_score(y_test,full_model.predict(X_test))\n",
    "full_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16afe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_score = accuracy_score(y_test,pca_model.predict(X_test_red))\n",
    "pca_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdabc49",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint</summary>\n",
    "    \n",
    "- Your PCA has the same number of training images, but 40 times less features\n",
    "- The features selected are the ones with the greatest variance, i.e. explain most of the difference between images\n",
    "- Thus, because of the curse of dimensionality, the performance of the full model should be less than the preformance of your PCA model\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1170caa",
   "metadata": {},
   "source": [
    "### üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfee3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('classification', full_score = full_score,\n",
    "                         pca_score=pca_score, ratio=time_ratio)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c2ecb",
   "metadata": {},
   "source": [
    "# Further improving our score\n",
    "\n",
    "Up to now, we have used `LogisticRegression` to predict our features, and a set value for our `n_components`. In this last part of the exercise, you will get a chance to further improve your classification score by doing the following:\n",
    "\n",
    "* Create a pipeline containing a `PCA()` and a `RandomForestClassifier()`\n",
    "* Tuning your pipeline using `GridSearchCV`\n",
    "\n",
    "I recommend tuning the following parameters with values you deem reasonable: for PCA the `n_components`, and for the RandomForestClassifier `max_depth` and `max_features`.\n",
    "\n",
    "Save your best `accuracy_score` for the `X_test` in a variable named `best_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e29490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('pca',PCA()),\n",
    "    ('lr',RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e94dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fae1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'lr__max_depth':[3, 7, 10],\n",
    "    'lr__max_features':[2, 10, 20],\n",
    "    'pca__n_components':[2,10,20, 200]\n",
    "}\n",
    "\n",
    "trained_rf = GridSearchCV(pipe, cv=5, param_grid=params, scoring='accuracy',\n",
    "                                n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c45e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf30b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = accuracy_score(y_test, trained_rf.predict(X_test))\n",
    "best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978a734e",
   "metadata": {},
   "source": [
    "### üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c94d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('full_pipeline', best_accuracy=best_score)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fafba",
   "metadata": {},
   "source": [
    "# üèÅ Finished!\n",
    "\n",
    "Well done! <span style=\"color:teal\">**Push your exercise to GitHub**</span>, and move on to the next one."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
