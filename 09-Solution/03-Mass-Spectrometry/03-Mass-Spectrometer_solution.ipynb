{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110b4d06",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-cL5eOpEsbuIEkvwW2KnpXC12-PAbamr\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a0695",
   "metadata": {},
   "source": [
    "# üß´ Finding Outliers in Mass Spectrometer Data\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1eIB1nVKS3u6GLEHxvAob9-XjW84PENJx\" style=\"width:600px\">\n",
    "\n",
    "Welcome to our group's **<a href=\"http://www.carbonateresearch.org/clumpedLab\">mass spectrometer lab!</a>** \n",
    "\n",
    "When we are not busy on a `machine learning` project, we work on reconstructing temperature from carbonate minerals using a technique known as <a href=\"http://www.carbonateresearch.org/clumped_isotope\"> Clumped Isotope Paleo-thermometry</a>. This is achieved by using a highly sensitive mass spectrometer, and measuring samples and `standards`.\n",
    "\n",
    "One of the issues with sensitive measurements is that many things can go wrong, and this can impact the quality of the data. This is especially problematic because the data needs to be corrected by the use of `external standards`: carbonate material with known values.\n",
    "\n",
    "In this exercise, you will use a range of `anomaly detection` methods to determine whether or not there are outliers in a set of standards. This is actual data, from our actual lab: in fact, it comes from the work of one of my PhD students. If you find an outlier, they will want to know!\n",
    "\n",
    "First, create a new DataFrame called `data` by loading the `mass_spectrometer.csv` file, and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3082d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='1iRkWXEUM8tnZkvP80RcueJLj7_6M8soJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1190121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('raw_data/mass_spectrometer.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75310f71",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "The data contains 6 different types of carbonate standards (*ETH1-4*, *IOL* and *Carrara Marble*), and we measured the following features:\n",
    "* `first_MX44_Ref_Gaz`: the *mV* measured on the reference side of the mass spectrometer: this could indicate reference with too small amount of gas\n",
    "* `first_MX44_Sample_Gaz`: Same as above, but for the sample side\n",
    "* `D47`: The clumped isotope value of isotopologues of mass 47 (this is the main isotopologue to be used to estimate the temperature of precipitation of the mineral)\n",
    "* `d13C`: The carbon isotope ratio of the standard\n",
    "* `d18O`: The oxygen isotope ratio of the standard\n",
    "* `name`: The name of the standard - essentially, a label\n",
    "\n",
    "## Expected values\n",
    "\n",
    "Because we are dealing with standards, we know what value we can expect. This information is in the `standards.csv` file: load it into a `standards` dataframe and have a look at the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7baa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "standards = pd.read_csv('raw_data/standards.csv').set_index('name')\n",
    "standards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e03a9",
   "metadata": {},
   "source": [
    "# Deviation from expected values\n",
    "\n",
    "One of the challenge we have is that we want to estimate if any given standard deviates from the expected values. But if we look at the raw values in `d13C`, `d18O`, and `D47`, each standard will have different values (the other features, `first_MZ44_Ref_Gas`, `first_MZ44_Sample_name` and `49_param`, are expected to be the same for all measured material). In order to determine if a measurement deviates from the means, we will need to calculate the difference between the measured sample and the column.\n",
    "\n",
    "Using the `data` Dataframe for the measured samples, and the expected standard values (the `standards` DataFrame) that I have given you above, create three new columns that reflect the difference between the measured values (`data`), and the expected value (`standards`): `d13C_diff`, `d18O_diff`, and `D47_diff`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad4fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['std_d13C', 'std_d18O', 'std_D47']] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db0d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in data.name.unique():\n",
    "    print(name)\n",
    "    for param in ['d13C','d18O', 'D47']:\n",
    "        idx = data[data.name==name].index\n",
    "        data.loc[idx,f'std_{param}'] = standards.loc[name, param]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb7cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['d13C_diff'] = data['d13C'] - data['std_d13C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd3826",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['d18O_diff'] = data['d18O'] - data[ 'std_d18O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['D47_diff'] = data['D47'] - data[ 'std_D47']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad72cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.name == 'IOL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f4a3f",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f3e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('check_df',\n",
    "                         df = data\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3528a59",
   "metadata": {},
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "We are nearly ready to do our investigation and see if we find any outliers. But first, we want to do a few things:\n",
    "\n",
    "* Create a `y` label baed on the `name` column, and use a `LabelEncoder` to encode it. This will allow us later to plot data with the type of sample as a unique color\n",
    "* Create a new `X` feature matrix that will contain the following features: `first_MZ44_Ref_Gas`, `first_MZ44_Sample`, `49_param`,  `d13C_diff`, `d18O_diff`, `D47_diff`. As you can see, we have dropped the original features and stick to the differences (except for three features)\n",
    "* Because many of our algorithms are sensitive to scale, we want to use a `scaler that transform our values in a range from 0 to 1` (choose well)\n",
    "\n",
    "Create the `y` label and the `X` feature as suggested above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c86bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder().fit(data.name)\n",
    "\n",
    "y = encoder.transform(data.name)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c6fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['first_MZ44_Ref_Gas', 'first_MZ44_Sample', \n",
    "       '49_param',  'd13C_diff',\n",
    "       'd18O_diff', 'D47_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6659ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = pd.DataFrame(MinMaxScaler().fit_transform(X), columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f784aa8e",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('data_prepared',\n",
    "                         data = X,\n",
    "                         label = y\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd16b73",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "We are now in a position where we can model potential outliers. Outlier detection can be subjective, so our strategy will be to use three different algorithms (`IsolationForest`, `OnceClassSVM`, and `HDBSCAN`) and combine their outputs to decide on what is an outlier.\n",
    "\n",
    "## `IsolationForest`\n",
    "\n",
    "Create an `IsolationForest` model, fit it, and predict your `X` feature (save the prediction into a variable called `ifor_pred`). Then, create a scatter plot of `D47_dff` versus `d13C_diff`, and use the `ifor_pred` as the color of each datapoint (look at the `c` parameter of `plt.scatter`).\n",
    "\n",
    "Do you think that the amount of outliers suggested by the `IsolationForest` is reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cbdffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "ifor = IsolationForest().fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb14d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifor_pred = ifor.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455eab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,8))\n",
    "\n",
    "\n",
    "ax.scatter(X['D47_diff'], X['d13C_diff'], c=ifor_pred);\n",
    "ax.set_xlabel('Difference in D47');ax.set_ylabel('Difference in d13C');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59cb28a",
   "metadata": {},
   "source": [
    "### A better visualization thanks to PCA\n",
    "\n",
    "The original space is not ideal to see outliers versus normal samples, because we used 6 features for outliers detection and need to plot this in 2 dimensions. A better approach is to perform a PCA analysis, and plot the data along its two principal components.\n",
    "\n",
    "Do the following:\n",
    "* create a PCA model, and `fit_transform` your `X` features into a new variable (`data_pca`)\n",
    "* Replot the `ifor_pred`, but this time use the `first principal component` and the `second principal component` of your data as you x and y for the scaller plot\n",
    "* Still use the `ifor_pred` as the `c` parameter\n",
    "\n",
    "Are things clearer this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8700f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA().fit(X)\n",
    "data_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc04134",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "\n",
    "ax.scatter(data_pca[:,0], data_pca[:,1], c=ifor_pred);\n",
    "ax.set_xlabel('Principal Component 1');ax.set_ylabel('Principal Component 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d429dda",
   "metadata": {},
   "source": [
    "<details><summary>Observations</summary><br>\n",
    "    We can indeed see a better separation between the outliers and the normal samples. However, we seem to have many outliers!</details>\n",
    "    \n",
    "### Ensuring that our outliers are not representing different samples\n",
    "\n",
    "One worry is that it would be possible, given that we have different samples, that the algorithm picks-up on valid sample differences. To ensure that this is not the case, replot the same plot as above, but this time using the `y` (effectively, the sample names) as your `c` paramter. Comparing the two, do you see any evidence that the outliers are simply different types of samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c6a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10, 10))\n",
    "ax.scatter(data_pca[:,0], data_pca[:,1], c=y);\n",
    "ax.set_xlabel('Principal Component 1', size=16);ax.set_ylabel('Principal Component 2', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44161bb",
   "metadata": {},
   "source": [
    "<details><summary>Observations</summary><br>\n",
    "    Although there is a little bit of separation of different samples along <code>principal component 2</code> we can clearly see that outliers belong to different samples. We do not seem to have an issue.</details>\n",
    "    \n",
    "## One-Class SVM\n",
    "\n",
    "Now, use a One-Class SVM to predict outliers, and save the results into `ocsvm_pred`. Plot the data in the `PCA` space, using `ocsvm_pred` as the color for your label. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc33db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "ocsvm_pred = OneClassSVM(nu=.05).fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbce41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12, 8))\n",
    "cm = plt.colormaps['RdYlBu']\n",
    "ax.scatter(data_pca[:,0], data_pca[:,1], c=ocsvm_pred, cmap=cm);\n",
    "ax.set_xlabel('Principal Component 1', size=16);ax.set_ylabel('Principal Component 2', size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ce982",
   "metadata": {},
   "source": [
    "<details><summary>Observations</summary><br>\n",
    "    Again, `OneClassSVM` seems to pick too many outliers. This could be sorted by setting the <code>nu</code> hyperparameter (effectively, the percentage of outliers) to something more reasonable, for instance 0.05 (5%). The default is 50%! Go ahead, change that and replot your data. </details>\n",
    "    \n",
    "## HDBSCAN\n",
    "\n",
    "Finally, we will use `HDBSCAN`. Remember that we can use the distance from clusters detected by `HDBSCAN` as a potential measure for outliers. Create an `HDBSCAN` model, and fit it with your `X` dataset. Then, plot your data into the `PCA` space, and use the `outlier_scores_` feature of your `HDBSCAN` fitted model as your `c` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c81e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "hdbscan_od = HDBSCAN().fit(X)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(12, 8))\n",
    "im = ax.scatter(data_pca[:,0], data_pca[:,1],c=hdbscan_od.outlier_scores_, cmap=cm); \n",
    "ax.set_xlabel('Principal Component 1', size=16);ax.set_ylabel('Principal Component 2', size=16);\n",
    "plt.colorbar(im, label='Outlier Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec136f1",
   "metadata": {},
   "source": [
    "### From outlier score to classification\n",
    "\n",
    "You will notice that `HDBSCAN` is different than the previous two models: the other two output either `1` (not an outlier) or `-1` (outlier), wherewas `HDBSCAN` outputs a continuous value from 0 to 1 that can be interpreted as a probability of the sample being an outlier.\n",
    "\n",
    "To be compatible with the other two metric, we will need to convert this score into a class. Go ahead and create a new variable named `dbscan_pred`: use a threshold of 0.7 to create outliers (`-1`) for any samples above this, and normal sample (`1`) for the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5ae254",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_pred = [-1 if x>0.7 else 1 for x in hdbscan_od.outlier_scores_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974dad5d",
   "metadata": {},
   "source": [
    "# Combining the output of all of our outlier detectors\n",
    "\n",
    "Combine the prediction of all three `outlier predictions` into one: only classify a sample as an outlier **if all three** predictions are that they are an outlier. Plot the data once again, and this time using this final prediction as the color for `c`.\n",
    "\n",
    "Does the final prediction make sense?\n",
    "\n",
    "<details><summary><strong>üí° Tip</strong></summary><br>\n",
    "    It would help if your prediction was in <code>Boolan</code> format (with the outliers labelled as <code>True</code>), at least later!</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc75c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = [True if x==-3 else False for x in (dbscan_pred+ifor_pred+ocsvm_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b07ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12, 8))\n",
    "cm = plt.colormaps['RdYlBu']\n",
    "im = ax.scatter(data_pca[:,0], data_pca[:,1],c=final_pred, cmap=cm); \n",
    "ax.set_xlabel('Principal Component 1', size=16);ax.set_ylabel('Principal Component 2', size=16);\n",
    "plt.colorbar(im, label='Outlier Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a5ace3",
   "metadata": {},
   "source": [
    "### Identifying the outliers\n",
    "\n",
    "Create a new dataframe called `outliers` that contains only the outliers: now you know which samples is suspicious! (also, if you followed my advice of turning your final classifier as a `boolean` array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d1e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = data[final_pred]\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2982a62",
   "metadata": {},
   "source": [
    "# Making sense of the outliers\n",
    "\n",
    "Can we make sense of the outliers? Let's do a scatter plot of the data, using the following criteria:\n",
    "* Plot the `49_param` vs `D47`. `49_param` (the 49 parameter) is a measure of contamination, and `D47` is the clumped isotope composition we are interested in\n",
    "* Plot your data in a large enough axis: I suggest a `figsize=(10,10)`\n",
    "* Plot the entire dataset with a symbol size of 120, and use a 10% transparency (`alpha=.1`)\n",
    "* On the same axis, plot the `outliers` but in full opaque mode (`alpha=1`) and a square marker\n",
    "* For both dataset, use the encoded `name` as your label (you might need to encode it for your `outliers`)\n",
    "\n",
    "If you follow the instruction above, you should be able to see the outliers right on top of the original data, and plotted with the same colors.\n",
    "\n",
    "What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba55319",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_outliers = encoder.transform(outliers.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "ax.scatter(data['49_param'],data['D47'], c=y, s=120, alpha=.1)\n",
    "ax.scatter(outliers['49_param'],outliers['D47'], marker='s',s=100, c=y_outliers)\n",
    "ax.set_xlabel('49 Parameter', size=18);ax.set_ylabel('D47 CDES', size=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64258c45",
   "metadata": {},
   "source": [
    "<details><summary><strong>Observations</strong></summary><br>\n",
    "    This requires some experience with the details of clumped isotopes, but if you have done this right, we can make sense of the outliers that are detected here:\n",
    "    <li> Two outliers plot with high <code>49_param</code>, indicating that they are probably contaminated and should be ignored</li>\n",
    "    <li> One outlier plots at around 0.2 for <code>D47</code>, and clearly has the wrong label: it has probably been mislabelled, and should be renamed in the database. This happens relatively often!</li>\n",
    "    <li> The last outlier plots at around 0.5 for <code>D47</code>, it could either be mislabelled (this could be investigated), it could be a little bit contaminated, or it could be fine and picked up as an outlier by accident. This require further investigation.</li>\n",
    "</details>\n",
    "\n",
    "# Conclusions\n",
    " \n",
    "Hopefully this has convinced you that outlier detection is not that hard, and very helpful to check your data. Keep in mind that this could be an initial step of a wider machine learning project.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94a7958",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba444a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('check_outliers',\n",
    "                         outliers = outliers\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1144eb29",
   "metadata": {},
   "source": [
    "# üèÅ Finished!\n",
    "\n",
    "Well done! <span style=\"color:teal\">**Push your exercise to GitHub**</span>, and move on to the next one."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
