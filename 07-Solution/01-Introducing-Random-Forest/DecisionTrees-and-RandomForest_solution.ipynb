{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ef7d51",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-cL5eOpEsbuIEkvwW2KnpXC12-PAbamr\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff3e3c",
   "metadata": {},
   "source": [
    "# Introduction to `RandomForest`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887ec53",
   "metadata": {},
   "source": [
    "The goal of this exercise will be to explore  `RandomForest` for Classification. We will use the same dataset about earthquakes in California that you used yesterday. We will show that `RandomForest` does better at classifying this data than the `neural networks`! And what's more, it is faster to train and much easier.\n",
    "\n",
    "# Importing the data\n",
    "I have already prepared and split the data for you, so there is no need to do any data preparation for this exercise. The preparation follows exactly my solution to the exercise from yesterday. Simply run the code to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734fc1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='1H_uTkFdNkY1FXllK5CmOjPpcqqd_PzkZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed312550",
   "metadata": {},
   "source": [
    "Now let's read the file into a `train` and `test` set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f409f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the files\n",
    "import pandas as pd\n",
    "\n",
    "X_train = pd.read_csv('raw_data/X_train_prep.csv')\n",
    "X_test = pd.read_csv('raw_data/X_test_prep.csv')\n",
    "\n",
    "y_train = pd.read_csv('raw_data/y_train.csv').values.ravel() # Ravel is used to change a column vector into a row vector\n",
    "y_test = pd.read_csv('raw_data/y_test.csv').values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59726582",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7856e0f4",
   "metadata": {},
   "source": [
    "### Creating a smaller set\n",
    "\n",
    "Training on 225k+ instances will take time. So we will experiment by creating a smaller version of our `X_train`. This is something to do with caution, as our algorithm will learn from the statistics of this train set and not from the entire dataset. But it will be much faster for some of the tests I want you to do. And we will eventually use the entire `X_train` and `X_test` for our algorithm.\n",
    "\n",
    "Let's resample our `X_train` to take only 20% of the dataset. The easiest it to create a new dataframe (`data`) that contains both `X_train` and `y_train` and sample it randomly. This gives us `X_train_small` and `X_test_small` (still containing about 45k instances):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef16b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X_train.copy()\n",
    "data['y'] = y_train.copy()\n",
    "\n",
    "data = data.sample(frac=.2,random_state = 42)\n",
    "\n",
    "X_train_small = data.drop(columns='y')\n",
    "y_train_small = data.y.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee6251",
   "metadata": {},
   "source": [
    "# GridSearch different algorithms\n",
    "\n",
    "Create a simple pipeline that contains a `classifier` parameter (you can set it up to be `RandomForestClassifier()`. Then, create a parameter grid that will automatically test this pipeline by switching the `classifier` between these algorithms: \n",
    "* `SVC()`\n",
    "* `DecisionTreeClassifier()`\n",
    "* `RandomForestClassifier()`\n",
    "* `LogisticRegression(max_iter=3000)` (the standard `max_iter` will throw errors)\n",
    "* `KNeiborsClassifier()`\n",
    "\n",
    "Feel free to add any other classifier to the list if you want. Run the `GridSearchCV` with `cv=5` and `scoring=\"precision\"`. Once the algorithm is fitted with `X_train_small` and `y_train_small`, check for the `best_score_`, and save the `best_params_` into a variable called `best_parameters`: so, which model won?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f92906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model_pipe = Pipeline([('classifier',RandomForestClassifier())])\n",
    "\n",
    "param_grid = [\n",
    "    {'classifier': [SVC(), DecisionTreeClassifier(), RandomForestClassifier(), LogisticRegression(max_iter=3000), KNeighborsClassifier()]}]\n",
    "\n",
    "grid_search = GridSearchCV(model_pipe, param_grid, cv=5,\n",
    "                           scoring='precision', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2138158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e299cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = grid_search.best_params_\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b83b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = grid_search.best_score_\n",
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f090a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae730f",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0501d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('best_algorithm',\n",
    "                         model = best_parameters,\n",
    "                         best_score = best_score\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1618c59",
   "metadata": {},
   "source": [
    "# A critical look\n",
    "\n",
    "You should have obtained a 95% precision with only 20% of the data we used to train our `MLPClassifier` last day. This is a very good performance! But remember what we learned about metrics on day 2? One metric alone can hide problems in our model.\n",
    "\n",
    "Let's do the following here: calculate the `precision_score` of your model on the `X_test`, and then the `recall_score`. Save both of these into variables `precision_score_rf` and `recall_score_rf`.  What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "precision_score_rf = precision_score(y_test, best_model.predict(X_test))\n",
    "recall_score_rf = recall_score(y_test, best_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f03cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0650ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76868c4",
   "metadata": {},
   "source": [
    "<details><summary>Conclusions</summary>\n",
    "Whilst the precision score is reasonably high, the recall score is <85%. If we pushed the training more, there is a risk that we would overfit to the data even more and end up with a very low recall. To avoid that, from now on we will use `f1` as our metric in `GridSearchCV`. This ensures that we improve both `precision` and `recall` together.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab1c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('balance_score',\n",
    "                         precision_score = precision_score_rf,\n",
    "                         recall_score = recall_score_rf\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecc4811",
   "metadata": {},
   "source": [
    " Narrowing our Hyperparameter Space with RandomSearchCV\n",
    "\n",
    "As you have discovered, for our dataset at least, `RandomForest` outperforms all other models.  Very often, `ensemble` methods will outperform other approaches, and on tabular data, it will even outperform deep-learning (and is much easier and quicker to train). So this needs to be part of your arsenal of methods!\n",
    "\n",
    "What we will do now is try to fine-tune our `RandomForest` algorithm by using `RandomizedSearchCV`. We have also established that if we want to avoid overfitting to `precision` (which would mean a very low `recall`, i.e. only picking up a few positive classes and leaving a lot behind), we are better off using `f1` as our metric for `RandomizedSearchCV`. I suggest you try the following combination of hyperparameters into `GridSearchCV`:\n",
    "\n",
    "* `max_depth` randomly between 1 and 50\n",
    "* `n_estimator` randomly between 100 and 500\n",
    "* `min_sample_split` randomly between 2 and 10\n",
    "* `max_features` randomly between 1 and 8\n",
    "\n",
    "This time, use the full `X_train` and `y_train` (not the smaller version). Run your `RandomizedSearchCV` with `cv=5`, `n_iter=10`(training is long because the dataset is large, and for this exercise we don't want to spend 30 minutes), scoring for accuracy, `random_state=42`,  and setting your `n_jobs=-1` to parallelize the work. Then, save the `best_score_` in a variable called `best_score` and the best algorithm (`best_estimator_`) as `rf_model`.\n",
    "\n",
    "Then, use your `X_test` to calculate the `recall_score` and `precision_score` (save these into the `rf_recall` and `rf_precision`, respectivly. Did you beat the standard hyperparameters?\n",
    "\n",
    "**Warning**: This code took a full 5 minutes to run on my machine, so be patient (or go take a break when it runs). Long running time for machine learning is normal, and you will learn this even more when you do deep-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distrib = [\n",
    "    {'classifier__max_depth': randint(low=1, high=50),\n",
    "     'classifier__n_estimators': randint(low=100, high=500),\n",
    "    'classifier__min_samples_split': randint(low=2, high=10),\n",
    "    'classifier__max_features': randint(low=1, high=8)}]\n",
    "\n",
    "rnd_search = RandomizedSearchCV(model_pipe, param_distributions=param_distrib, cv=5, n_iter=10,\n",
    "                           scoring='f1', n_jobs=-1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b417532",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = rnd_search.best_score_\n",
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c5c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = rnd_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_precision = precision_score(y_test,rf_model.predict(X_test))\n",
    "rf_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "rf_recall = recall_score(y_test,rf_model.predict(X_test))\n",
    "rf_recall "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4f2059",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('randomized_search_score',\n",
    "                         rf_precision = rf_precision,\n",
    "                         rf_recall = rf_recall\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9da186",
   "metadata": {},
   "source": [
    "# Why Ensemble Learning Works\n",
    "\n",
    "The way your `RandomForestClassifier` algorithm reached a decision is by pooling the predictions of many individual `DecisionTreeClassifiers` together: the class that was voted most often was selected. Ensemble method work because they rely on the fact that the classification error of weak learners (such as `DecisionTreeClassifiers`) is random, and thus if the experiment is repeated often the error distribution will be guassian, with the mean centered on the correct value.\n",
    "\n",
    "We can convince ourselves of that by looking at individual `DecisionTreeClassifiers` and pooling their results.\n",
    "\n",
    "First, create a new pipeline (I call it `dtree`) that combines your `preproc` pipeline with a `DecisionTreeClassifier()` algorithm. Then, run a `cross_validate` loop on `dtree` using the `X_train_small` and `y_train_small` (for speed), and a `cv=200`. Save the result of this into a variable (`cv`) and plot the histogram of the `test_score`. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a17cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv = cross_validate(DecisionTreeClassifier(), X_train_small, y_train_small, cv=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a888e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(cv['test_score']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d634b56d",
   "metadata": {},
   "source": [
    "<details><summary>üå≥ Observations</summary><br>\n",
    "    We can see that the distribution of the scores is more or less Gaussian, but each individual tree performs very differently, some with very high scores (overfitting) and some with very low scores. Most of the trees have a precision close to 98%</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0495f4",
   "metadata": {},
   "source": [
    "## Ensembling the trees\n",
    "\n",
    "We can do a pseudo-ensemble very simply by using the `cross_val_predict` function located in the same module as `cross_validate`. The `cross_val_predict` function will return the average prediction (in our case, class) during cross validation. Run `cross_val_predict` using the same parameters as for `cross_validate` above (use `X_train_small` and `y_train_small`), and save the results in a variable (say, `cv_predict`). Then, run an `accuracy_score` function using `y_train` and `cv_predict`, and save this as `cv_score`: are we close to the `RandomForestClassifier` score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d3c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "cv_predict = cross_val_predict(DecisionTreeClassifier(), X_train_small, y_train_small, cv=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "cv_score = precision_score(y_train_small, cv_predict)\n",
    "cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c0270",
   "metadata": {},
   "source": [
    "<details><summary>üå≥ Observations</summary><br>\n",
    "    The score should be around 84-85% accuracy, which is less than our <code>RandomForestClassifier</code>. Ultimately, <code>RandomForestClassifier</code> will work better because of the more advanced <code>bootstrapping</code> of the data and the ability to randomly select different <code>features</code> for each tree: this increases the variance of the trees, and thus results in stronger predictive performance than what we can so with a simple cross-validation</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad9c7d3",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "With `DecisionTreeClassifiers`, it is very easy to see how a decison is made: simply follow the branches of the decision trees, and it will be obvious why a given sample belongs to a particular class. However, by pooling together many trees, `RandomForestClassifiers` loose this simple interpretability. The field of `Explainable AI` is an expanding one: in many fields (think law, medicine, finance) being able to explain why you reach a certain decision is just as important as the decision itself.\n",
    "\n",
    "Explainable AI is beyond the the topic of this module, but we can at least explore the `feature importance` parameters of `RandomForest`, which is based on the `Gini impurity` index and a weighted factor based on the `probability of a given class being reached` on the tree. More details about the mathematics of this <a href=\"https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3#:~:text=Feature%20importance%20is%20calculated%20as,the%20more%20important%20the%20feature.\">can be found in this excellent Medium post</a>. \n",
    "\n",
    "Using your previous `rf_model` (best estimator) that you saved above, access the `classifier` (remember: `GridSearchCV` and `RandomSearchCV` results can be accessed as dictionaries) output the `feature_importances_` values: a bigger value means more importance.\n",
    "\n",
    "To know what value corresponds to what variable is easy: create a new `pd.Series` called `f_imp` wiht the `data` being the `feature_importance_` and the columns being the columns of `X_train`, and sort this series in descending order.\n",
    "\n",
    "Now you know what features contribute to the decision, and what feature do not!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_imp = pd.Series(rf_model['classifier'].feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "f_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7811832",
   "metadata": {},
   "source": [
    "## Note on decision trees probabilities\n",
    "\n",
    "As with `LogisticRegression`, you can output a classification probability from `DecisionTrees` by using the `predict_proba()` function. Try this below for your `X_test` - you should see, for each sample, a list of 2 probabilities (one per class) and the largest is the one selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f30f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c8bfe",
   "metadata": {},
   "source": [
    "However, <strong style=\"color:red\">unlike LogisticRegression</strong> the probability of `DecisionTrees` is **not calibrated**: this means it represent the probability of a sample belonging to the given node of the tree, but **NOT** the probability of the sample distribution. `LogisticRegression` in that sense is better, though there has been some efforts to improve the probabilities of uncalibrated model by, for instance, combining logistic regression and decision trees (see <a href=\"https://gdmarmerola.github.io/probability-calibration/\"> a very interesting post about this here</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f48504",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe250896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('trees',\n",
    "                         score = cv_score, \n",
    "                         importance = f_imp\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5a050",
   "metadata": {},
   "source": [
    "# üèÅ You are all done!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
