{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f452904",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1-hPP-XPm9_5M3orUgmompcVleQ5xvPST\" style=\"Width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f672a4",
   "metadata": {},
   "source": [
    "# Introducing the `XGBoost` Library\n",
    "\n",
    "We will continue working on our Earthquake damage predictions, but this time, we will use a different approach: `Boosted Trees` with `XGBoost`.\n",
    "\n",
    "`XGBoost` (this stands for `eXtreme Gradient BOOSTing`), and it is one of the most popular machine learning library. Many `Kaggle` competitions are won using `XGBoost`. This library is dedicated to `ensemble learning` with decision trees, and it is very complete and has many options. It can handle `RandomForest` classification, but also `BoostedTrees` by performing `AdaBoost` on a `RandomForest` classifier. \n",
    "\n",
    "I highly recommend you <a href=\"https://xgboost.readthedocs.io/en/stable/\">use the very complete the documentation</a> of `XGBoost` if you are serious about applying machine learning to tabular data: it is one of the most powerful family of algorithms at the moment.\n",
    "\n",
    "Part of the success of `XGBoost` is that it leverages principles from `Deep-learning` and applies the approach to `DecisionTrees`: for instance, you can control the `learning rate` of the `boosted trees` in `XGBoost`, something you are familiar with through your use of the `SGDClassifier` and `SGDRegressor` classes, and our lecture on Wednesday of week 1.\n",
    "\n",
    "Here, we will see that `XGBoost` will outperform all of the classifiers we developped in our previous notebook.\n",
    "\n",
    "\n",
    "# Opening the data\n",
    "\n",
    "As in the previous exercise, open the data in `earthquake_nepal.csv`,  separate the target (`damage_grade`) and the features, and do a train_test_split with 80% in the `train_set` (use a `random_state=42` to be consistent with my results). <br>\n",
    "Convert your `y_train` and `y_test` to categorical data using a `label_encoder`.\n",
    "\n",
    "**Note**: this time, I am giving you more data. In fact, one order of magnitude more data! But you will see that `XGBoost` can handle this relatively easily, and this will contribute to boosting our performance. We should achieve an **accuracy > 70%**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3692c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbta.utils import download_data\n",
    "download_data(id='1q2id-UaRkjFRm_bCefaAPXONI5hB15mK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c767cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing the dataset\n",
    "data = pd.read_csv('raw_data/earthquake_nepal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c01b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = data.drop(columns='damage_grade')\n",
    "y = data.damage_grade\n",
    "\n",
    "l_enc = LabelEncoder()\n",
    "\n",
    "y = l_enc.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351bf860",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7251d47",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb08df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('data_preproc',\n",
    "                         X_shape = X_train.shape,\n",
    "                         y_values = pd.DataFrame(y_train).value_counts().shape[0],\n",
    "                         y_type = type(pd.DataFrame(y_train).value_counts().index[0][0])\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4706992",
   "metadata": {},
   "source": [
    "# Open the saved `preproc` pipeline\n",
    "\n",
    "Now, using `joblib`, load your saved `preproc` pipeline, save it in a variable named `preproc`, and transform your `X_train` and `X_test` with this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "preproc = load('preproc.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ccb33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preproc.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05619ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = preproc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11649579",
   "metadata": {},
   "source": [
    "# Create a validation set\n",
    "\n",
    "First of all, as we would do when training neural networks, `XGBoost` can use a `validation_set` to track the performance of our algorithm over time. So let's create an `X_val` and `y_val` by further splitting our `X_train` and `y_train` into an 80%/20% `X_train` `y_train` / `X_val` `y_val` (don't forget to save the new `X_train` and `y_train` with the same names, or your `X_val` and `y_val` sets will be also present in your `X_train` `y_train` and thus you will have a data leak):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a81c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d61b73",
   "metadata": {},
   "source": [
    "# XGBoost: A gentle Introduction\n",
    "\n",
    "Let's get acquainted with the basic use of the `XGBClassifier` class. Import it from the `xgboost` library, and run train a plain-vanilla version of the classifier. Save the accuracy score into a variable called `xgb_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ffcec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d45af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_test, xgb.predict(X_test))\n",
    "xgb_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfba5bd",
   "metadata": {},
   "source": [
    "<details><summary>üîç Observations</summary><br>\n",
    "    Right off the bat, the <code>XGBoost</code> classifier performs as strongly as our strongest previous classifier!</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fff8a6",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('base_accuracy',\n",
    "                         score = xgb_accuracy\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cfc2f3",
   "metadata": {},
   "source": [
    "### `XGBoost` hyperparameters\n",
    "\n",
    "Now, let's explore some of the basic hyperparameters of `XGBoost`. We will focus here on the ones that are particular to the algorithm, and not the ones similar to `DecisionTrees`. The hyperparameters we will play with include:\n",
    "\n",
    "* `n_estimators`: the number of boosting round we want our algorithm to go through.\n",
    "* `learning_rate`: the learning rate for the gradient descent algorithm\n",
    "* `early_stopping_rounds`: determines after how many boosting rounds training will stop if no improvement is detected\n",
    "\n",
    "To be able to use the `early_stopping_rounds` we need to pass both our training set (the `X_train` and `y_train`) and our evaluation set (the `X_val` and `y_val`) as a `eval_set` argument of our `fit()` function.\n",
    "‚ö†Ô∏è This needs to be passed as a **list of (X, y) tupples**. \n",
    "\n",
    "We also need to set the `eval_metric` to reflect a valid cost function for classification (`\"accuracy\"` cannot be a cost function) when we create our `XGBClassifier`.\n",
    "\n",
    "To get a feel of how `XGBoost` works with a validation set, go ahead and  create a new `XGBClassifier` with `n_estimators=100`, a `eval_metric` of `\"mlogloss\"` (multinomial log-loss), a `learning_rate=1.8`,  and pass it your `train` and `validation` sets during fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84464e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(eval_metric='mlogloss', n_estimators=100, learning_rate=1.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train, y_train, eval_set=[(X_train, y_train),(X_val, y_val)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4daff8a",
   "metadata": {},
   "source": [
    "## ‚òùÔ∏è Training performance at each iteration: learning curve\n",
    "\n",
    "As you can see, `XGBoost` is outputing a log of the `losses` both for your training (`validation_0` if you passed `(X_train, y_train)` first in your `eval_set`) and your validation set (`validation_1` if you passed `(X_train, y_train)` first in your `eval_set`).\n",
    "\n",
    "If you remember last week's lecture on training curves (and I hope you do!) you will recognise the value of this data: we can plot a learning curve, and see what our algorthm is doing.\n",
    "\n",
    "**Do the following:**\n",
    "* See what the `evals_result()` function returns when applied to your trained `XGBClassifier` (*tip*: it returns the training loop losses for both of your validation sets). Save this into a variable called `results`\n",
    "* Explore `results`: it is a dictionary-like objects, so see what the keys are, and what the keys of the returned object are.\n",
    "* Once you understand your `results`, write a simple python function that will draw both training curves on a plot. I suggest using a `figsize` of 15, 10, or something similar to show the curve nicely.\n",
    "* Draw the learning curve for the `XGBClassifier` that you trained above\n",
    "\n",
    "Then, based on your observations, answer the following question:\n",
    "* Does the algorithm `'underfit'`, `'overfit'`, or is it `'balanced'`?\n",
    "\n",
    "Save your answer as a string in a variable named `performance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = xgb.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bdf9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['validation_0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_learning_curves(results):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(15,10))\n",
    "\n",
    "    ax.plot(results['validation_0']['mlogloss'], c='blue', label='Train set');\n",
    "    ax.plot(results['validation_1']['mlogloss'], c='orange', label='Validation set');\n",
    "    ax.legend();\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_learning_curves(results);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83adebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = 'overfit'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377e139d",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('fitting',\n",
    "                         performance = performance\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623d682",
   "metadata": {},
   "source": [
    "## Changing Hyperparameters\n",
    "\n",
    "Now, let's change the hyperparameters of the `XGBClassifier` to:\n",
    "* `eval_metric='mlogloss'` \n",
    "* `n_estimators=150` \n",
    "* `learning_rate=0.3`\n",
    "\n",
    "Retrain your model, and draw the learning curve. What do you think your model is doing now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019770dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(n_estimators=150, eval_metric='mlogloss', learning_rate=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108313b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train, y_train, eval_set=[(X_train, y_train),(X_val, y_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dbde76",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_learning_curves(xgb.evals_result());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e90b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, xgb.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a08b34d",
   "metadata": {},
   "source": [
    "<details><summary><strong>üí° Observations and strategy</strong></summary><br>\n",
    "    You should see now that the training is much smoother, and that your validation set is showing some steady decrease.</details>\n",
    "    \n",
    "## Controlling overfitting\n",
    "\n",
    "Now, let's control overfitting by tweaking two hyperparameters:\n",
    "* Create a new `XGBClassifier` with the exact same hyperparameters as above\n",
    "* set the new `min_child_weight` parameter to `6`\n",
    "* set the new `max_depth` parameter to `7`\n",
    "* retrain the model, and check the learning curve.\n",
    "* Calculate an `accuracy_score` and save it under a variable named `final_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d36ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(n_estimators=150, \n",
    "                    eval_metric='mlogloss', \n",
    "                    learning_rate=0.3,\n",
    "                   min_child_weight=6,\n",
    "                   max_depth=7)\n",
    "\n",
    "xgb.fit(X_train, y_train, eval_set=[(X_train, y_train),(X_val, y_val)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb07220",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_learning_curves(xgb.evals_result());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0220049",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = accuracy_score(y_test, xgb.predict(X_test))\n",
    "final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2163b8",
   "metadata": {},
   "source": [
    "<details><summary><strong>üí° Observations and strategy</strong></summary><br>\n",
    "    We are improving a little bit on our model, but not very much. Feel free to do a grid search or other hyperparameter search if you want to.</details>\n",
    "    \n",
    "# How did I come up with these hyperparameters?\n",
    "\n",
    "So, how did I come up with these sets of `hyperparameters` to improve on our model? Simply put, I used `hyperparameter search`, in particular the `BayesCV` from the <a href=\"https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html\">scikit-optimize</a> library.\n",
    "\n",
    "But this takes a lot of time, so rather than have you do it in this notebook, I did it for you. But how do you start optimizing a complex algorithm like `XGBoost`? This is not easy, because there are many hyperparameters to tweak.\n",
    "\n",
    "A good way to start is by <a href=\"https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\">following this article that explains how to do hyperparameter tuning</a> for `XGBoost`: there is a clear guidelines on which ones to optimize first, and which ones to do only later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb5b54",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccbf3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('final_score',\n",
    "                         score = final_score\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f140952",
   "metadata": {},
   "source": [
    "# üèÅ Finished!\n",
    "\n",
    "Well done! <span style=\"color:teal\">**Push your exercise to GitHub**</span>, and move on to the next one."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
